\subsection{Dual Linearity (Theory): Neural Path Kernel}\label{sec:theory}





%Our goal is to characterise `what is learnt in the gates of a DNN with ReLUs'. To this end, we use the DGN setup where the gates and weights are separate. Specifically, the DNN with ReLUs whose gates we want to examine is the feature network in the DGN. We then measure the information in the gates by keeping the gates fixed and training the value network and measuring the test performance of the value network. It was shown in [\citenum{npk}] that if the gates are fixed  (i.e., with reference to \Cref{prop:ntks}, $\kf=0$), for fully connected DGN, the NTK simplifies as $\text{NTK}(x,x')\propto \text{NPK}(x,x') = \ip{x,x'}\cdot {\bf{overlap}}(x,x')$. In this section, we state three results, where in \Cref{th:main} we `re-write' Theorem~$5.1$ in [\citenum{npk}] to explicitise the role of gates, width and depth, to yield a simple product of kernels expression (unnoticed in prior works).  \Cref{th:mainconv} covers the case of convolutions with pooling and \Cref{th:mainres} covers the case residual networks with skip connections.

%\textbf{Note.} The gates of the DGN can also be tuned, in which case $\kf\neq \mathbf{0}$, and we reserve the study of $\kf$ for future. The results are for infinite width networks and at randomised initialisation. We use these insights as an indicator of what we can expect when we experiment with finite width networks. 

%We begin with an assumption that states that the value network weights have to be statistically independent of the feature network weights at initialisation. %With reference to \Cref{prop:ntks}, since the gates fixed, it follows that $\kf=\mathbf{0}$ and we will characterise only $\kv$ which corresponds to the value network. Here, the DGN is used to study the gates of a \emph{untrained/(partially or fully) trained} DNN with ReLUs. We wish to point that the gates of the DGN can also be tuned in which case $\kf\neq \mathbf{0}$, and we reserve the study of $\kf$ for future. We now state an assumption on the value network 

%In this section, we state three results (\Cref{th:main,th:mainconv,th:mainres}) that characterise `what is learnt in the gates of a DNN with ReLUs'.  To this end, we use the DGN setup where the gates and weights are separate. Specifically, the DNN with ReLUs whose gates we want to examine becomes the feature network in the DGN. If we keep the feature network weights fixed, the gates are automatically fixed. We then characterise the information in the gates by training the value network and measuring the test performance of the value network. With reference to \Cref{prop:ntks}, since the gates fixed, it follows that $\kf=\mathbf{0}$ and we will characterise only $\kv$ which corresponds to the value network. Here, the DGN is used to study the gates of a \emph{untrained/(partially or fully) trained} DNN with ReLUs. 

