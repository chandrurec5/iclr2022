
\section{Convolution With Global Average Pooling}\label{sec:conv}
In this section, we define NPFs and NPV in the presence of convolution with pooling. This requires three key steps (i) treating pooling layers like gates/masks (see \Cref{def:pooling}) (ii) bundling together the paths that share the same path value
(due to weight sharing in convolutions, see \Cref{def:bundle}),  and (iii) re-defining the NPF and NPV for bundles (see \Cref{def:convnps}). Weight sharing due to convolutions and pooling makes the NPK rotationally invariant \Cref{lm:cnnnpk}. We begin by describing the architecture.

\textbf{Architecture:} We consider (for sake of brevity) a $1$-dimensional\footnote{The results follow in a direct manner to any form of circular convolutions.} convolutional neural network with circular convolutions, with $\dc$ convolutional layers ($l=1,\ldots,\dc$), followed by a \emph{global-average-pooling} layer ($l=\dc+1$) and $\dfc$ ($l=\dc+2,\ldots,\dc+\dfc+1$) fully connected  layers. The convolutional window size is $\wconv<\din$, the number of filters per convolutional layer as well as the width of the FC is $w$. 

\textbf{Indexing:} Here $\iin/\iout$ are the indices (taking values in $[w]$) of the input/output filters. $\icin$ denotes the indices of the convolutional window taking values in $[\wconv]$. $\ifout$ denotes the indices (taking values in $[\din]$, the dimension of input features) of individual nodes in a given output filter. The weights of layers $l\in[\dc]$ are denoted by $\Theta(\icin,\iin,\iout,l)$ and for layers $l\in[\dfc]+\dc$ are denoted by $\Theta(\iin,\iout,l)$. The pre-activations, gating and hidden unit outputs are denoted by $q_{x,\Theta}(\ifout,\iout,l)$,  $G_{x,\Theta}(\ifout,\iout,l)$, and $z_{x,\Theta}(\ifout,\iout,l)$ for layers $l=1,\ldots, \dc$.

\begin{definition}[Circular Convolution]
For $x\in\R^{\din}$, $i\in[\din]$ and $r\in\{0,\ldots,\din-1\}$, define :

(i) $i\oplus r = i+r$, for $i+r \leq \din$ and $i\oplus r =i+r-\din$, for $i+r>\din$.

(ii) $rot(x,r)(i)=x(i\oplus r), i\in[\din]$.

(iii) $q_{x,\Theta}(\ifout,\iout,l)=\sum_{\icin,\iin}\Theta(\icin,\iin,\iout,l)\cdot z_{x,\Theta}(\ifout\oplus (\icin-1),\iin,l-1)$. 
\end{definition}
\begin{definition}[Pooling]\label{def:pooling}
Let $G^{\text{pool}}_{x,\Theta}(\ifout,\iout,\dc+1)$ denote the pooling mask, then we have
\centerline{
$z_{x,\Theta}(\iout, \dc+1) =\sum_{\ifout} z_{x,\Theta}(\ifout,\iout,\dc)\cdot G^{\text{pool}}_{x,\Theta}(\ifout,\iout,\dc+1),$
}
where in the case of \emph{global-average-pooling} $G^{\text{pool}}_{x,\Theta}(\ifout,\iout,\dc+1)=\frac{1}{\din},\forall \iout\in[w], \ifout\in[\din]$.
\end{definition}
\FloatBarrier
\begin{table}[!h]
\centering
\resizebox{1.0\columnwidth}{!}{
\begin{tabular}{|c l lll|}\hline
Input Layer&: &$z_{x,\Theta}(\cdot,1,0)$ &$=$ &$x$ \\\hline
\multicolumn{5}{l}{\quad }\\
\multicolumn{5}{l}{\quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad Convolutional Layers, $l\in[\dc]$}\\\hline
%\multicolumn{5}{l}{\quad }\\\hline
Pre-Activation&: & $q_{x,\Theta}(\ifout,\iout,l)$& $=$ & $\sum_{\icin,\iin}\Theta(\icin,\iin,\iout,l)\cdot z_{x,\Theta}(\ifout\oplus (\icin-1),\iin,l-1)$\\
Gating Values&: &$G_{x,\Theta}(\ifout,\iout,l)$& $=$ & $\mathbf{1}_{\{q_{x,\Theta}(\ifout,\iout,l)>0\}}$\\
Hidden Unit Output&: &$z_{x,\Theta}(\ifout,\iout,l)$ & $=$ & $q_{x,\Theta}(\ifout,\iout,l)\cdot G_{x,\Theta}(\ifout,\iout,l)$\\\hline
\multicolumn{5}{l}{\quad }\\
\multicolumn{5}{l}{\quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad GAP Layer, $l=\dc+1$}\\\hline
%HUO&: &${z}_{x,\Theta}(\iout,l)$ & $=$ & $\frac{1}{\din}\sum_{i\in [\din]} z_{x,\Theta}(i,\iout,l-1)$\\\hline\hline
Hidden Unit Output&: &$z_{x,\Theta}(\iout, \dc+1)$ & $=$ &$\sum_{\ifout} z_{x,\Theta}(\ifout,\iout,\dc)\cdot G^{\text{pool}}_{x,\Theta}(\ifout,\iout,\dc+1)$\\\hline
\multicolumn{5}{l}{\quad }\\
\multicolumn{5}{l}{\quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad Fully Connected Layers, $l\in[\dfc]+(\dc+1)$}\\\hline
Pre-Activation&: & $q_{x,\Theta}(\iout,l)$& $=$ & $\sum_{\iin}\Theta(\iin,\iout,l) \cdot z_{x,\Theta}(\iin,l-1) $\\
Gating Values&: &$G_{x,\Theta}(\iout,l)$& $=$ & $\mathbf{1}_{\{(q_{x,\Theta}(\iout,l))>0\}}$\\
Hidden Unit Output&: &$z_{x,\Theta}(\iout,l)$ & $=$ & $q_{x,\Theta}(\iout,l)\cdot G_{x,\Theta}(\iout,l)$\\
Final Output&: & $\hat{y}_{\Theta}(x)$ & $=$ & $\sum_{\iin}\Theta(\iin,\iout, d)\cdot z_{x,\Theta}(\iin,d-1)$\\\hline
\end{tabular}
}
\caption{Shows the information flow in the convolutional architecture described at the beginning of \Cref{sec:conv}.}
\label{tb:cconv}
\end{table}




\subsection{Neural Path Features, Neural Path Value}

\begin{proposition}
The total number of paths in a CNN is given by  $\Pcnn=\din(\wconv w)^{\dc}w^{(\dfc-1)}$.
\end{proposition}

\begin{notation}[Index Maps]
The ranges of index maps $\Ifeat_l$,  $\Iconv_l$, $\I_l$ are $[\din]$, $[\wconv]$ and $[w]$ respectively. 
\end{notation}

\begin{definition}[Bundle Paths of Sharing Weights]\label{def:bundle}
Let $\hat{P}^{\text{cnn}}=\frac{\Pcnn}{\din}$, and $\{B_1,\ldots, B_{\hat{P}^{\text{cnn}}}\}$ be a collection of sets such that $\forall i,j\in [\hat{P}^{\text{cnn}}], i\neq j$ we have $B_i\cap B_j=\emptyset$ and $\cup_{i=1}^{\hat{P}^{\text{cnn}}}B_i =[\Pcnn]$. Further,  if paths $p,p' \in B_i$, then $\Iconv_l(p)=\Iconv_l(p'), \forall l=1,\ldots, \dc$ and $\I_l(p)=\I_l(p'), \forall l=0,\ldots, \dc$.
\end{definition}

\begin{proposition}\label{prop:bundle}
There are exactly $\din$ paths in a bundle.
\end{proposition}

\begin{definition}\label{def:convnps} Let $x\in\R^{\din}$ be the input to the CNN. For this input, 
\begin{tabular}{rlp{12cm}}
$A_{\Theta}(x,p)$&$\eqdef$&$\left(\Pi_{l=1}^{\dc+1} G_{x,\Theta}(\Ifeat_l(p),\I_l(p),l)\right)\cdot\left(\Pi_{l=\dc+2}^{\dc+\dfc+1} G_{x,\Theta}(\I_l(p),l)\right)$\\
$\phi_{x,\Theta}(\hat{p})$&$\eqdef$&$ \sum_{\hat{p}\in B_{\hat{p}}}x(\Ifeat_0(p))A_{\Theta}(x,p)$\\
$v_{\Theta}(B_{\hat{p}})$&$\eqdef$&$ \left(\Pi_{l=1}^{\dc} \Theta(\Iconv_{l}(p),\I_{l-1}(p),\I_{l}(p),l)\right) \cdot\left( \Pi_{l=\dc+2}^{\dc+\dfc+1} \Theta(\I_{l-1}(p),\I_l(p),l)\right)$ 
\end{tabular}
\begin{center}
\begin{tabular}{|c|c|}\hline
NPF &$\phi_{x,\Theta}\eqdef (\phi_{x,\Theta}(B_{\hat{p}}),\hat{p}\in [\hat{P}^{\text{cnn}}])\in\R^{\hat{P}^{\text{cnn}}}$\\\hline
NPV& $v_{\Theta}\eqdef (v_{\Theta}(B_{\hat{p}}),\hat{p}\in [\hat{P}^{\text{cnn}}])\in\R^{\hat{P}^{\text{cnn}}}$\\\hline
\end{tabular}
\end{center}
\end{definition}


\subsection{Rotational Invariant Kernel}
\begin{lemma}\label{lm:cnnnpk}
\begin{align*}
\text{NPK}^{\texttt{CONV}}_{\Theta}(x,x')&=\sum_{r=0}^{\din-1} \ip{x,rot(x',r)}_{\textbf{overlap}_{\Theta}(\cdot, x,rot(x',r))}\\&=\sum_{r=0}^{\din-1} \ip{rot(x,r),x'}_{\textbf{overlap}_{\Theta}(\cdot, rot(x,r),x')}
\end{align*}
\end{lemma}

\begin{proof}
For the CNN architecture considered in this paper, each bundle has exactly $\din$ number of paths, each one corresponding to a distinct input node. For a bundle $b_{\hat{p}}$, let $b_{\hat{p}}(i),i\in[\din]$ denote the path starting from input node $i$.
\begin{align*}
&\sum_{\hat{p}\in [\hat{P}]} \Bigg(\sum_{i,i'\in[\din]} x(i) x'(i') A_{\Theta}\left(x,b_{\hat{p}}(i)\right) A_{\Theta}\left(x',b_{\hat{p}}(i')\right) \Bigg)\\
=&\sum_{\hat{p}\in [\hat{P}]}\Bigg(\sum_{i\in[\din],i'=i\oplus r, r\in\{0,\ldots,\din-1\}} x(i) x'(i\oplus r) A_{\Theta}\left(x,b_{\hat{p}}(i)\right) A_{\Theta}\left(x',b_{\hat{p}}(i\oplus r)\right)\Bigg)\\
=&\sum_{\hat{p}\in [\hat{P}]}\Bigg(\sum_{i\in[\din], r\in\{0,\ldots,\din-1\}} x(i) rot(x',r)(i) A_{\Theta}\left(x,b_{\hat{p}}(i)\right) A_{\Theta}\left(rot(x',r),b_{\hat{p}}(i)\right)\Bigg)\\
=&\sum_{r=0}^{\din-1} \Bigg(\sum_{i\in[\din]} x(i) rot(x',r)(i) \sum_{\hat{p}\in [\hat{P}]}  A_{\Theta}\left(x,b_{\hat{p}}(i)\right) A_{\Theta}\left(rot(x',r),b_{\hat{p}}(i)\right)\Bigg)\\
=&\sum_{r=0}^{\din-1}\Bigg(\sum_{i\in[\din]} x(i) rot(x',r)(i) \textbf{overlap}_{\Theta}(i,x,rot(x',r))\Bigg)\\
=&\sum_{r=0}^{\din-1} \ip{x,rot(x',r)}_{\textbf{overlap}_{\Theta}(\cdot,x,rot(x',r))}
\end{align*}
\end{proof}


In what follows we re-state \Cref{th:conv}.

\begin{theorem} Let $\sigcnn=\frac{\cscale}{\sqrt{w\wconv}}$ for the convolutional layers and $\sigfc=\frac{\cscale}{\sqrt{w}}$ for FC layers. Under \Cref{assmp:main}, as $w\rightarrow\infty$, with  $\bcnn = \ \left(\dconv \sigcnn^{2(\dconv-1)}\sigfc^{2\dfc}+\dfc \sigcnn^{2\dconv}\sigfc^{2(\dfc-1)}\right)$ we have:
\begin{align*}
&\text{NTK}^{\texttt{CONV}}_{\Tdgn_0}\rightarrow\quad \frac{\bcnn}{{\din}^2} \cdot \text{NPK}^{\texttt{CONV}}_{\Tf_0}
\end{align*}
\end{theorem}

\begin{proof}
Follows from Theorem~5.1 in [\citenum{npk}].
\end{proof}

\section{Residual Networks with Skip connections}

As a consequence of the skip connections, within the ResNet architecture there are $2^b$ sub-FC networks (see \Cref{def:subfcdnn}). The total number of paths $\Pres$ in the ResNet is equal to the summation of the paths in these $2^b$ sub-FC networks (see \Cref{prop:resnetpath}). Now, The neural path features and the neural path value are $\Pres$ dimensional quantities, obtained as the concatenation of the NPFs and NPV of the $2^b$ sub-FC networks. 

\begin{proposition}\label{prop:resnetpath}
The total number of paths in the ResNet is  $\Pres = \din \cdot\sum_{i=0}^b \binom{b}{i} w^{(i+2)\dblock-1}$.
\end{proposition}

\begin{lemma}[Sum of Product Kernel]\label{lm:sumofproduct}
Let $\text{NPK}^{\texttt{RES}}_{\Theta}$ be the NPK of the ResNet, and $\text{NPK}^{\J}_{\Theta}$ be the NPK of the sub-FCNs within the ResNet obtained by ignoring those skip connections in the set $\J$. Then, \begin{align*}\text{NPK}^{\texttt{RES}}_{\Theta}=\sum_{\J\in 2^{[b]}}\text{NPK}^{\J}_{\Theta}\end{align*}
%\begin{align*}
%\end{align*}
\end{lemma}
\begin{proof}
Proof is complete by noting that the NPF of the ResNet is a concatenation of the NPFs of the $2^b$ distinct sub-FC-DNNs within the ResNet architecture.
\end{proof}

We re-state \Cref{th:res}
\begin{theorem} Let $\sigma=\frac{\cscale}{\sqrt{w}}$. Under \Cref{assmp:main}, as $w\rightarrow\infty$,  for $\bres^{\J} = (|\J| +2)\cdot\dblock\cdot \sigma^{2\big( (|\J|+2)\dblock-1\big)}$,
\begin{align*}
\text{NTK}^{\texttt{RES}}_{\Tdgn_0}\rightarrow \sum_{\J\in 2^{[b]}}  \bres^{\J} \text{NPK}^{\J}_{\Tf_0}
\end{align*}
\end{theorem}


\begin{proof}
Follows from Theorem~5.1 in [\citenum{npk}].
\end{proof}


\section{Numerical Experiments}\label{sec:expdetails}
We now list the details related to the numerical experiments which have been left out in the main body of the paper.

 $\bullet$ \textbf{Computational Resource.} The numerical experiments were run in Nvidia-RTX 2080 TI GPUs and Tesla V100 GPUs.

$\bullet$ All the models in Table I of \Cref{fig:c4gap} we used Adam \citep{adam} with learning rate of $3\times 10^{-4}$, and batch size of 32.

$\bullet$ In \Cref{sec:dlgn}, the codes for experiments based on VGG-16 and Resnet-110  were refactored from following repository: ``https://github.com/gahaalt/resnets-in-tensorflow2".

$\bullet$ For VGG-16-DLGN in \Cref{fig:c4gap} and DLGN-SF in \Cref{fig:shallow}, the $\max$-pooling were replaced by \emph{average} pooling so as to ensure that the feature network is entirely linear. For the comparison to be fair, we replaced the $\max$ pooling in VGG-16 reported in \Cref{fig:c4gap} by \emph{average} pooling. Batch normalisation layers were retained in VGG-16, VGG-16-DLGN and VGG-16-DLGN-SF (all three are shown in \Cref{fig:vggnets}).


$\bullet$ For VGG-16-DLGN in \Cref{fig:c4gap} and DLGN-SF in \Cref{fig:shallow}, the $\max$-pooling were replaced by \emph{average} pooling so as to ensure that the feature network is entirely linear. For the comparison to be fair, we replaced the $\max$ pooling in VGG-16 reported in \Cref{fig:c4gap}. Batch normalisation layers were retained in VGG-16, VGG-16-DLGN and VGG-16-DLGN-SF.

$\bullet$ DLGN of Resnet-110 was derived from Resnet-110 in a similar manner.

$\bullet$ All the VGG-16, Resnet-110 (and their DGN/DLGN) models in Table II of \Cref{fig:c4gap} we used \emph{SGD} optimiser with momentum $0.9$ and the following learning rate schedule (as suggested in ``https://github.com/gahaalt/resnets-in-tensorflow2") : for iterations $[0, 400)$ learning rate was $0.01$,  for iterations $[400, 32000)$ the learning rate was $ 0.1$, for iterations $[32000, 48000)$ the learning rate was $0.01$, for iterations $[48000, 64000)$ the learning rate was $0.001$. The batch size was $128$. The models were trained till $32$ epochs.

$\bullet$ The VGG-16-DLGN-SF in Table III of \Cref{fig:shallow} uses the same optimiser, batch size and learning rate schedule as the models in Table II of \Cref{fig:c4gap} as explained in the previous point.

$\bullet$ For C1GAP and C4GAP in Table III of \Cref{fig:shallow}, we used Adam \citep{adam} with learning rate of $10^{-3}$, and batch size of 32. This learning rate is best among the set $\{10^{-1},10^{-2},10^{-3}, 3\times 10^{-4}\}$ for C1GAP.
\newpage
\section{Models Used in \Cref{fig:c4gap} and \Cref{fig:shallow}}

%\Cref{fig:perm1} shows the permutations $1-12$  of C4GAP-DLGN, and \Cref{fig:perm1} shows the permutations $13-24$  of C4GAP-DLGN in Table I of \Cref{fig:c4gap}. \Cref{fig:vggnets} shows VGG-16, VGG-16-DLGN and VGG-16-DLGN-SF.

\begin{figure}[!b]
\centering
\begin{minipage}{0.48\columnwidth}
\centering
\resizebox{0.99\columnwidth}{!}{
\input{fig-c4dlgn-1}
}
\end{minipage}
\begin{minipage}{0.48\columnwidth}
\centering
\resizebox{0.99\columnwidth}{!}{
\input{fig-c4dlgn-2}
}
\end{minipage}

\begin{minipage}{0.48\columnwidth}
\centering
\resizebox{0.99\columnwidth}{!}{
\input{fig-c4dlgn-3}
}
\end{minipage}
\begin{minipage}{0.48\columnwidth}
\centering
\resizebox{0.99\columnwidth}{!}{
\input{fig-c4dlgn-4}
}
\end{minipage}


\begin{minipage}{0.48\columnwidth}
\centering
\resizebox{0.99\columnwidth}{!}{
\input{fig-c4dlgn-5}
}
\end{minipage}
\begin{minipage}{0.48\columnwidth}
\centering
\resizebox{0.99\columnwidth}{!}{
\input{fig-c4dlgn-6}
}
\end{minipage}



\begin{minipage}{0.48\columnwidth}
\centering
\resizebox{0.99\columnwidth}{!}{
\input{fig-c4dlgn-7}
}
\end{minipage}
\begin{minipage}{0.48\columnwidth}
\centering
\resizebox{0.99\columnwidth}{!}{
\input{fig-c4dlgn-8}
}
\end{minipage}

\begin{minipage}{0.48\columnwidth}
\centering
\resizebox{0.99\columnwidth}{!}{
\input{fig-c4dlgn-9}
}
\end{minipage}
\begin{minipage}{0.48\columnwidth}
\centering
\resizebox{0.99\columnwidth}{!}{
\input{fig-c4dlgn-10}
}
\end{minipage}

\begin{minipage}{0.48\columnwidth}
\centering
\resizebox{0.99\columnwidth}{!}{
\input{fig-c4dlgn-11}
}
\end{minipage}
\begin{minipage}{0.48\columnwidth}
\centering
\resizebox{0.99\columnwidth}{!}{
\input{fig-c4dlgn-12}
}
\end{minipage}

\caption{Shows the permutations $1-12$ C4GAP-DLGN in Table I of \Cref{fig:c4gap}. The top left is the identity permutation and is the vanilla model.}
\label{fig:perm1}
\end{figure}

\begin{figure}
\centering
\begin{minipage}{0.48\columnwidth}
\centering
\resizebox{0.99\columnwidth}{!}{
\input{fig-c4dlgn-13}
}
\end{minipage}
\begin{minipage}{0.48\columnwidth}
\centering
\resizebox{0.99\columnwidth}{!}{
\input{fig-c4dlgn-14}
}
\end{minipage}

\begin{minipage}{0.48\columnwidth}
\centering
\resizebox{0.99\columnwidth}{!}{
\input{fig-c4dlgn-15}
}
\end{minipage}
\begin{minipage}{0.48\columnwidth}
\centering
\resizebox{0.99\columnwidth}{!}{
\input{fig-c4dlgn-16}
}
\end{minipage}


\begin{minipage}{0.48\columnwidth}
\centering
\resizebox{0.99\columnwidth}{!}{
\input{fig-c4dlgn-17}
}
\end{minipage}
\begin{minipage}{0.48\columnwidth}
\centering
\resizebox{0.99\columnwidth}{!}{
\input{fig-c4dlgn-18}
}
\end{minipage}



\begin{minipage}{0.48\columnwidth}
\centering
\resizebox{0.99\columnwidth}{!}{
\input{fig-c4dlgn-19}
}
\end{minipage}
\begin{minipage}{0.48\columnwidth}
\centering
\resizebox{0.99\columnwidth}{!}{
\input{fig-c4dlgn-20}
}
\end{minipage}

\begin{minipage}{0.48\columnwidth}
\centering
\resizebox{0.99\columnwidth}{!}{
\input{fig-c4dlgn-21}
}
\end{minipage}
\begin{minipage}{0.48\columnwidth}
\centering
\resizebox{0.99\columnwidth}{!}{
\input{fig-c4dlgn-22}
}
\end{minipage}

\begin{minipage}{0.48\columnwidth}
\centering
\resizebox{0.99\columnwidth}{!}{
\input{fig-c4dlgn-23}
}
\end{minipage}
\begin{minipage}{0.48\columnwidth}
\centering
\resizebox{0.99\columnwidth}{!}{
\input{fig-c4dlgn-24}
}
\end{minipage}
\caption{Shows the permutations $13-24$ C4GAP-DLGN in Table I of \Cref{fig:c4gap}.}
\label{fig:perm1}
\end{figure}


\begin{figure}
\centering
\begin{minipage}{0.3\columnwidth}
\resizebox{!}{20cm}{
\input{fig-vgg}
}
\end{minipage}
\begin{minipage}{0.3\columnwidth}
\resizebox{!}{20cm}{
\input{fig-vgg-dlgn}
}
\end{minipage}
\begin{minipage}{0.3\columnwidth}
\resizebox{!}{20cm}{
\input{fig-vgg-dlgn-sl}
}
\end{minipage}
\caption{Shows VGG-16 (left), VGG-16-DLGN (middle), VGG-16-DLGN-SF(right).}
\label{fig:vggnets}
\end{figure}
