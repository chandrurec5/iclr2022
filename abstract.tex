\begin{abstract}
%We present an improved and simplified understanding of  deep neural network (DNNs) with rectified linear units (ReLUs). In particular, we focus on the gating property (i.e., \emph{on/off} state) of ReLU, due to which, for each input there is an \emph{active/on} sub-network comprising of those gates which are \emph{on} and the weights between those gates. Recently, \cite{npk} developed a \emph{dual view} (for dense networks) to separate the gates and the weights. They showed that most information is in the gates, and captured the role of the active sub-networks analytically via the so called \emph{neural path kernel} (NPK). In this paper, we simplify the NPK by expressing it explicitly in terms of the \emph{correlation of gates}, and derive the additional properties of NPK in the presence of convolutions and skip connections.  We experimentally verify that operations destroying the layer-by-layer structure such as permuting the layers, arbitrarily tiling and rotation of the gates and providing a constant input do not degrade performance, because, in all these operations, the correlation of gates is not lost. We also show experimentally that (open question related to) the degradation in test accuracy due to upstream training with random labels is because of the degradation to gates, thereby reinforcing the importance of gates. Thirdly, we modify standard architectures (VGG19 and a ResNet) to yield two deep gated networks in which feature extraction is free of activations and is separate from the gates and the weights -- these achieve greater than $90\%$ test accuracy on CIFAR-10. The other two theoretical results extend the dual view to cover the cases of convolutions with pooling and skip connections.
%Our main message is that the gates hold most useful information. 
We present a simplified and improved understanding of the inner workings of deep neural networks (DNNs) with rectified linear units (ReLUs) by focussing on the gating (i.e., `on/off' states) of the ReLUs. We build on prior work by \cite{npk} which also focussed on the role of gates in DNNs with ReLUs.  Our main claim is that gates are indeed the most fundamental entities in such DNNs that hold most useful information. We provide theoretical basis for the claim and experimental justification. Based on this simplified understanding, we conceptualise a DNN with ReLU to have three functional components (i) gating, (ii) pre-activation generation and (iii) weights.  In a  DNN with ReLU these three functionalities are shared/entangled between its weights and activation. We propose a novel modification to disentangle these three components thereby making the deep network entirely interpretable. We show that applying this modification on standard state of the art DNNs makes them entirely interpretable without significant loss of performance.
%we propose a novel modification that yields an entirely interpretable deep network.
 
%We present an improved and simplified understanding of  deep neural network (DNNs) with rectified linear units (ReLUs) by focussing on the gating property (i.e., on/off state) of ReLU.  We build on the \emph{dual view} introduced by \cite{npk}. The key simplification is the claim that DNNs with ReLUs are characterised by the \emph{correlation of gates}. We verify this claim by showing that operations destroying the layer-by-layer structure such as permuting the layers, arbitrarily tiling and rotation of the gates and providing a constant input do not degrade performance, because, in all these operations, the correlation of gates is not lost. We then take up an open question related to the degradation of test accuracy due to upstream training with random labels for study. Using the dual view, we show that this degradation is attributed to the gates thereby demonstrating the importance of the role of gates and efficacy of the dual view in understanding DNNs with ReLUs. Based on our improved understanding, we propose a novel modification that improves `interpretability' :  here (i) feature extraction, (ii) gating and (iii) weights are decoupled. We show on standard architectures that this novel modification achieves greater than $90\%$ and close to $70\%$ test accuracies on CIFAR-10 and CIFAR-100 respectively while improving interpretability.
\end{abstract}




%for a given pair of inputs is equal to the product of the inner product of the inputs and the sizes of the overlapping active sub-networks corresponding to the inputs.

