\section{Numerical Experiments}\label{sec:dlgn}
\begin{figure}[!b]
\centering
\begin{minipage}{0.40\columnwidth}
\centering
\resizebox{0.99\columnwidth}{!}{
\input{fig-c4dgn-permute}
}
\end{minipage}
\begin{minipage}{0.40\columnwidth}
\centering
\resizebox{0.99\columnwidth}{!}{
\input{fig-c4dlgn-permute}
}
\end{minipage}
%\begin{minipage}{0.30\columnwidth}
%\centering
%\resizebox{0.99\columnwidth}{!}{
%\input{fig-c4dslgn-permute}
%}
%\end{minipage}
\caption{DGN and DLGN of C4GAP. Here $i_1,i_2,i_3,i_4$ are permutation of $\{1,2,3,4\}$.}
\label{fig:c4gap}
\end{figure}

In this section, we show experimental results comparing performance of standard DNNs with their DGN and DLGN counterparts on CIFAR-10 and CIFAR-100. In what follows, we use the notation DGN$(\xf,\xv)$ and DLGN$(\xf,\xv)$ where $\xf$ and $\xv$ denote the input to the value and feature networks respectively. For instance, DGN$(x,x)$ will mean that both the value and feature network of the DGN is provided with the image as input, and DLGN$(x,\mathbf{1})$ will mean that the feature network is given with the image as input and the value network is given with $\mathbf{1}$ as input. We consider $3$ DNN architectures, the C4GAP (a $4$ layer convolution network with $128$ filters in each layer followed by global-average-pooling, see \Cref{fig:dgn,fig:c4gap}) VGG-16 and ResNet-110. %VGG-16 and ResNet-110 are chosen for their state of the art performance on standard datasets namely CIFAR-10 and CIFAR-100. 

\textbf{Training.} In all the $3$ architectures, the DGN and DLGN are trained from scratch, i.e., both the feature and value network are initialised at random and trained using off the shelf optimisers. In DGN and DLGN, we use soft gating (see \Cref{fig:dgn}) so that gradient flows through the feature network and the gates are learnt (we chose $\beta=10$). For experiments in \Cref{tb:permute}, we used \emph{Adam} optimiser, with learning rate of $3\times 10^{-4}$ and batch size of $32$. For  experiments in \Cref{tb:permute}, we used  \emph{SGD} optimiser with momentum of $0.9$ and trained for $64000$ steps with batch size of $128$ per step. The learning rate schedule was $0.01$ for first $400$ steps, and $0.1$ for steps $401$ to $3200$ and $0.01$ for steps $32001$ to $48000$ and $0.001$ for steps $48001$ to $64000$.

\textbf{Result I.} The choice of C4GAP is motivated by: (i) there only $4!=24$ permutations; this enables us to run all the permutations and (ii) ensure continuity with \citep{arora2019exact,npk}.  \Cref{tb:expresults} demonstrates the fact that computations in the weight/value network is \emph{disentangled in the path space}, i.e., destroying the layer-by-layer structure due to permutation or providing a constant $\mathbf{1}$ input does not degrade the performance.  Similar results hold for CIFAR-100 as well and are shown in the Appendix.

\textbf{Result II.} As claimed in the \Cref{sec:intro}, results in \Cref{tb:expresults} show that the DLGN recovers more than $83.5\%$ (i.e., $83.78\%$ in the worst case) of the performance of the DNN, which implies that while entanglement in the DNNs enable their improved performance,  the `disentangled and interpretable'  computations in the DLGN can still recover most part of the performance. %This result also implies that the most important function of gating is to \emph{lift} the linear computations in the feature network to the dual linear computations in the value network.

\textbf{Interpretation of DLGN$(x,\mathbf{1})$.} The feature network generates pre-activations via linear transformations, the pre-activations trigger the gates, the gates lift the computations to the dual linear space, which in the infinite width limit implements the product kernel $\Pi_{l=1}^{(d-1)} \frac{\ip{G_l(x),G_l(x')}}{w}$. Given that the value network is best interpreted as the product kernel, we can ask the following open questions about the feature network.

\textbf{Open Questions.} The following questions are still open: (i) how the gates are learnt?, (ii) what are learnt in the gates? (iii) why the non-linearity in the feature network helps (the performance gap between DGN and DNN is less compared to DLGN and DNN)? and (iv) what is the visual/human interpretation? While these questions are open, we believe that the fact that the feature network in a DLGN is a deep linear network, will make these questions amenable to techniques from standard linear algebra and tools from `filter bank' theory.  %Note that all these questions are related to the feature network, because, we have shown that the role of the gates and the value network is to obtain the gating features and realise the corresponding NPK.



%Recall that in a DLGN, the gating/feature network is a deep linear network without non-linearities, and hence is disentangled by construction. We claimed in \Cref{sec:intro} that the weight/value network is \emph{disentangled in the path space}, for which, as promised we presented theoretical insights in \Cref{sec:analysis}, and now we will provide the supporting experimental evidence. 



%will experimentally compared DNNs with their DGN and DLGN counterparts, to show how the `disentangled and interpretable' computations in a DLGN recover major part of the performance of DNNs. Recall that the gating/


%This also implies that while non-linearity in ReLU causes entanglement (and hence non-interpretability), the major function of the ReLU as gates is to lift the layer-by-layer computations in the gating/feature network to the path-by-path computations in the weight/value network. 
 

%In this section, we turn to the issues of entanglement and `black box'-ness. We start by noting that the performance in the \emph{learnable gates} setting of the DGN show us that the DGN is not only conceptually similar to the DNN, but also empirically approximate to the DNN. In this paper, we modify the DGN to obtain the LGLN. In particular, the LGLN (\Cref{fig:lgln}) is same as the DGN in which (i) the ReLUs in the feature network are replaced with identity activations, and (ii) the input to the value network is $\mathbf{1}$ (see bottom network of LGLN in \Cref{fig:lgln} and the input to the value network of the DGN in \Cref{fig:dgn}). We now discuss how the LGLN disentangles the computations into two linear structures to show that the commonly held view of sophisticated structures are learnt in a layer-by-layer manner is misconceived.  From now on, as it is with the DGN, we will refer to the top part of the LGLN as its feature network and the bottom part as its value network. 



%Continuing with the `learning in gates + learning in the weights given gates' interpretation, using LGLN (\Cref{fig:lgln}), we disentangle both `learning in the gates' as well as `learning in the weights given gates'. 


%We now elaborate on why these changes are needed in the LGLN and how disentangling is achieved.

%$\bullet$ \textbf{Disentangling Feature Network.}  %The gates themselves encode binary information and are triggered by their pre-activations.In a DGN, the transformation from the input to the pre-activation to the gates happens through the hidden layers, in which the linear and non-linear operations are entangled. The hidden layers are not interpretable, and we have to fall back to sophisticated structures being learnt in the hidden layers as an explanation for why it is possible to learn useful pre-activations (as hence useful gates). In LGLN, the non-linear ReLUs are removed, and the transformation in linear, i.e., while the learning of pre-activations happens in a layer-by-layer manner, the learnt structures are no longer sophisticated and are interpretable in terms of standard linear algebra. We will call this \textbf{primal linearity}. Primal linearity is ensured via construction itself and needs no further theoretical justification, and in what follows, we experimentally verify in \Cref{sec:exp} that linearly learnt pre-activations does not degrade performance significantly, when compared to state of the art.

%$\bullet$ \textbf{Disentangling Value Network.} %Note that the value network of the LGLN and that of the DGN are the same, except that in the DGN the value network is given $x\in\R^{\din}$ as the input and in the LGLN it is a constant $\mathbf{1}\in\R^{\din}$ instead.  We show via experiments in \Cref{sec:exp} that destroying the layer-by-layer structure and the constant $\mathbf{1}$ does not degrade performance. These results are counter intuitive, surprising, and more importantly difficult to reconcile with using the `sophisticated features are learnt in a layer-by-layer' explanation. However, we argue that these surprising results can be readily explained using the NPK expression in \Cref{th:fcprev}.  Note that in a DGN, learning the weights with fixed gates amounts to learning a linear model in the dual variables, i.e., learning $\hat{y}_{\text{DGN}}(x)=\ip{\phi_{\Tf}(x), v_{\Tv}}$. We called this \textbf{dual linearity} and in \Cref{sec:prelim} showed that dual linearity has a kernel interpretation in terms of the NPK in \Cref{th:fcprev}. 
%Thus, giving a constant $\mathbf{1}$ input would mean that $\ip{x,x'}=\din$, and the expression on the right reduces to $d \cdot \sigma^{2(d-1)}\cdot \din\cdot \textbf{overlap}(x,x')$. Now a previously unnoticed fact is that $\textbf{overlap}(x,x')$ is invariant to layer permutations, which implies destroying the layer-by-layer structure does not hurt. In short, the main role of the gating non-linearity is to lift the computations from the primal to the dual space. In the dual space, the right way to interpret the computations is path-by-path and the question of sophisticated structures being learnt in the layers is irrelevant.
%However, in terms of the dual variables, the constant $\mathbf{1}$ has the following interpretation: the neural path value is a vector specifying the contribution of each path to the final output and the neural path feature is a vector specifying the path activity (from \Cref{def:npf-npv} $\phi(x,p)=x\cdot A(x,p)$, and since input is $1$, we have $\phi(x,p)=A(x,p)$), i.e., which path to be selected and which one to be left out in the output. The destruction of layers is possible due to the permutation invariance property of the NPK itself which was unnoticed in prior work. 

%In what follows, in \Cref{th:fc}, we first restate Theorem 5.1 in \cite{npk} (i.e., \Cref{th:fcprev}) so as to explicitly capture the permutation invariance. In order to build a  more complete picture, we extend the dual view to cover the cases of convolution with global pooling and skip connections. We then present the aforementioned experimental results in \Cref{sec:exp}.

%What happens when we give $\mathbf{1}$ input to value network?

%What happens when we give permute the gates?

%Interpreting DLGN$(x,\mathbf{1})$

%Interpreting DSLGN$(x,\mathbf{1})$

%Power of Depth Alone


\begin{comment}
\begin{figure}
\centering
\begin{minipage}{1.0\columnwidth}
\centering
\resizebox{0.99\columnwidth}{!}{
\input{fig-c4gap-permute}
}
\resizebox{0.99\columnwidth}{!}{
\input{fig-c4gap-no-act-permute}
}
\end{minipage}
\caption{$4$ convolutional layers with GAP}
\label{fig:c4gap}
\end{figure}




\begin{figure}[!t]
\centering
\begin{minipage}{1.0\columnwidth}
\centering
\begin{minipage}{0.49\columnwidth}
\resizebox{0.99\columnwidth}{!}{
\input{fig-c4dgngap-permute}
}
\end{minipage}
\begin{minipage}{0.49\columnwidth}
\resizebox{0.99\columnwidth}{!}{
\input{fig-c4lglngap-permute}
}
\end{minipage}
\end{minipage}
\caption{$4$ convolutional layers with GAP}
\label{fig:c4gap}
\end{figure}
\end{comment}

\begin{table}[!t]
\centering
\resizebox{1.0\columnwidth}{!}{
\begin{tabular}{cccccccc}
\toprule 
Dataset 					& C4GAP 		&DNN 				&DGN$(x,x)$ 			&DGN$(x,\mathbf{1})$ 	&DLGN$(x,x)$ 			&DLGN$(x,\mathbf{1})$ 	&Recovery\\\midrule
\multirow{2}{*}{CIFAR10}		& Vanilla		&80.5\tiny{$\pm$0.4} 	&77.4\tiny{$\pm$0.3} 	& 77.5\tiny{$\pm$0.2} 	&75.4\tiny{$\pm$0.3} 	&75.4\tiny{$\pm$0.2}		&$93.66\%$	\\
						& Permute 	&-- 					&77.3\tiny{$\pm$0.5} 	&77.9\tiny{$\pm$0.6}		&75.9\tiny{$\pm$0.5} 	&76.0\tiny{$\pm$0.5}		&$94.40\%$	\\					%\midrule
%\multirow{2}{*}{CIFAR100}		& Vanilla  				&51.8\tiny{$\pm$0.4} 	&47.4\tiny{$\pm$0.2}		&47.3\tiny{$\pm$0.3} 	&47.4\tiny{$\pm$0.1} 	&48.0\tiny{$\pm$0.2}		\\
%						& Permute 	& -- 		&48.4\tiny{$\pm$0.8} 	&49.2\tiny{$\pm$0.9} 	&47.5\tiny{$\pm$1.0} 	&48.4\tiny{$\pm$0.9}		\\
\bottomrule
\end{tabular}
}
\caption{\small{Top row shows results for vanilla models without permutations; the results are averaged over $5$ runs. The bottom row shows results of $4!-1=24-1=23$ permutations (other than the identity permutation) for each model; the results are averaged over the $23$ permutations.}}
\label{tb:permute}
\end{table}

\begin{table}[!t]
\centering
\resizebox{1.0\columnwidth}{!}{
\begin{tabular}{cccccccccc}
\toprule 
Dataset 					& Model 		&DNN 				&DGN$(x,x)$ 			&DGN$(x,\mathbf{1})$ 	&DLGN$(x,x)$ 			&DLGN$(x,\mathbf{1})$ 	&Recovery\\\midrule		
\multirow{2}{*}{CIFAR10}		&VGG16 		&93.6\tiny{$\pm$0.2} 	& 93.0\tiny{$\pm$0.1}  	&93.0\tiny{$\pm$0.1}   	&87.0\tiny{$\pm$0.1}		&87.0\tiny{$\pm$0.2}		&$92.94\%$	\\
						&ResNet110 	&94.0\tiny{$\pm$0.2} 	& 93.3\tiny{$\pm$0.2} 	&93.2\tiny{$\pm$0.1} 	&87.9\tiny{$\pm$0.2}   	&87.8\tiny{$\pm$0.1} 	&$93.40\%$	\\\midrule
\multirow{2}{*}{CIFAR100}		&VGG16	 	&73.4\tiny{$\pm$0.3}  	&70.3\tiny{$\pm$0.1} 	&70.5\tiny{$\pm$0.2} 	&61.5\tiny{$\pm$0.2}		&61.5\tiny{$\pm$0.1}		&$\mathbf{83.78\%}$	\\
						&ResNet110 	&72.7\tiny{$\pm$0.2}		&70.8\tiny{$\pm$0.2} 	&70.8\tiny{$\pm$0.4}		&62.3\tiny{$\pm$0.2} 	&62.7\tiny{$\pm$0.3} 	&$86.24\%$	\\
\bottomrule
\end{tabular}
}
\caption{Summary of Experiments}
\label{tb:expresults}
\end{table}


\begin{comment}
\begin{table}[!t]
\centering
\resizebox{1.0\columnwidth}{!}{
\begin{tabular}{ccccccccc}
\toprule 
Dataset 					& Model 				&DNN 				&DGN$(x,x)$ 			&DGN$(x,\mathbf{1})$ 	&DLGN$(x,x)$ 			&DLGN$(x,\mathbf{1})$ 	&DSLGN$(x,x)$ 		&DSLGN$(x,\mathbf{1})$  \\\midrule
\multirow{2}{*}{CIFAR10}		& C4GAP 				&80.5\tiny{$\pm$0.4} 	&77.4\tiny{$\pm$0.3} 	& 77.5\tiny{$\pm$0.2} 	&75.4\tiny{$\pm$0.3} 	&75.4\tiny{$\pm$0.2}		&74.2\tiny{$\pm$0.2}	 	&73.3\tiny{$\pm$0.2} \\
						& C4GAP-PERMUTE 	&-- 					&77.3\tiny{$\pm$0.5} 	&77.9\tiny{$\pm$0.6}		&75.9\tiny{$\pm$0.5} 	&76.0\tiny{$\pm$0.5}		& -- 					& -- \\\midrule
\multirow{2}{*}{CIFAR100}		& C4GAP 				&51.8\tiny{$\pm$0.4} 	&47.4\tiny{$\pm$0.2}		&47.3\tiny{$\pm$0.3} 	&47.4\tiny{$\pm$0.1} 	&48.0\tiny{$\pm$0.2}		&45.8\tiny{$\pm$0.3}	 	&44.9\tiny{$\pm$0.1}	\\
						& C4GAP-PERMUTE 	& -- 					&48.4\tiny{$\pm$0.8} 	&49.2\tiny{$\pm$0.9} 	&47.5\tiny{$\pm$1.0} 	&48.4\tiny{$\pm$0.9}		& --					& -- \\
\bottomrule
\end{tabular}
}
\caption{Summary of Experiments}
\label{tb:premresults}
\end{table}

\begin{table}[!t]
\centering
\resizebox{1.0\columnwidth}{!}{
\begin{tabular}{ccccccccc}
\toprule 
Dataset 					& Model 		&DNN 				&DGN$(x,x)$ 			&DGN$(x,\mathbf{1})$ 	&DLGN$(x,x)$ 			&DLGN$(x,\mathbf{1})$ 	&DSLGN$(x,x)$ 		&DSLGN$(x,\mathbf{1})$  \\\midrule		
\multirow{2}{*}{CIFAR10}		&VGG16-AP 	&93.6\tiny{$\pm$0.2} 	& 93.0\tiny{$\pm$0.1}  	&93.0\tiny{$\pm$0.1}   	&87.0\tiny{$\pm$0.1}		&87.0\tiny{$\pm$0.2}		&84.8\tiny{$\pm$0.3} 	&84.9\tiny{$\pm$0.2} \\
						&ResNet110 	&94.0\tiny{$\pm$0.2} 	& 93.3\tiny{$\pm$0.2} 	&93.2\tiny{$\pm$0.1} 	&87.9\tiny{$\pm$0.2}   	&87.8\tiny{$\pm$0.1} 	&72.4{$\pm$13.8} 		&71.2{$\pm$17.3}  \\\midrule
\multirow{2}{*}{CIFAR100}		&VGG16-AP 	&73.4\tiny{$\pm$0.3}  	&70.3\tiny{$\pm$0.1} 	&70.5\tiny{$\pm$0.2} 	&61.5\tiny{$\pm$0.2}		&61.5\tiny{$\pm$0.1}		&56.4\tiny{$\pm$0.4} 	&56.3\tiny{$\pm$0.2}\\
						&ResNet110 	&72.7\tiny{$\pm$0.2}		&70.8\tiny{$\pm$0.2} 	&70.8\tiny{$\pm$0.4}		&62.3\tiny{$\pm$0.2} 	&62.7\tiny{$\pm$0.3} 	&49.8{$\pm$0.9}		&41.5{$\pm$15.5}  \\
\bottomrule
\end{tabular}
}
\caption{Summary of Experiments}
\label{tb:expresults}
\end{table}
\end{comment}

\subsection{Open Question: Upstream training with random labels}\label{sec:exp2}
\textbf{Open Question [\citenum{randlabel}]).} {When trained with random labels upstream followed by true labels downstream, the test performance of a DNN with ReLUs degrades, Why?}

\textbf{Answer Using Dual View}. Let us denote training by true labels by subscript $\text{T}$ and training by random labels first followed by true labels by subscript RT. In what follows, we also drop $\Theta$ from the notation. When trained with true labels, the DNN learns the relation $\hat{y}(x)=\ip{\phi_{\text{T}}(x),v_{\text{T}}}$. The scenario in the open question corresponds to learning  $\hat{y}(x)=\ip{\phi_{\text{RT}}(x),v_{\text{RT}}}$. In order to disentangle the effect of the random labels on the gates and the weights separately, we consider the following two scenarios: 

(i) learning $\hat{y}(x)=\ip{\phi_{\text{RT}}(x),v_{\text{RT}}}$; this can be achieved using the DGN setup by pre-training the feature network first with random labels followed by true labels, and then training the value network with true labels.

(ii) learning $\hat{y}(x)=\ip{\phi_{\text{T}}(x),v_{\text{RT}}}$; this can be achieved using the DGN setup by pre-training the feature network with true labels, and then training the value network first with random labels followed by true labels.
\begin{table}[!t]
\centering
%\resizebox{1.0\columnwidth}{!}{
\begin{tabular}{cccc}
\toprule 
$\hat{y}(x)=\ip{\phi_{\text{T}}(x),v_{\text{T}}}$ 	&$\hat{y}(x)=\ip{\phi_{\text{T}}(x),v_{\text{RT}}}$ 	&$\hat{y}(x)=\ip{\phi_{\text{RT}}(x),v_{\text{RT}}}$ 	&$\hat{y}(x)=\ip{\phi_{\text{RT}}(x),v_{\text{T}}}$ 	\\\midrule
81.0\tiny{$\pm$0.3}						&79.9\tiny{$\pm$0.3}							&68.5\tiny{$\pm$0.4}							&67.7\tiny{$\pm$0.4}								\\
\bottomrule
\end{tabular}
%}
\caption{\small{Random labels affect the gates. In column $2$ the random labels were corrupted by $100\%$. In columns $3$ and $4$ the random labels were corrupted by $75\%$, because we could not train the DNN with $100\%$ corrupt labels.}}
\label{tb:randlabel}
\end{table}

\textbf{Result.} From \Cref{tb:randlabel}, it is evident from column $2$ that if the gates, i.e., the neural path features are \emph{clean}, training the weights, i.e., the neural path values with random labels first followed by true labels degrades test performance by $2\%$. Also, it is evident from column $4$ that if the gates, i.e., the neural path features are \emph{corrupt}, training the weights, i.e., the neural path values with true labels still have poor performance. Based on these, we conclude that, \emph{when trained with random labels upstream followed by true labels downstream, the test performance of a DNN with ReLUs degrades, because, the gates degrade.}

%& C4GAP 		& 51.8 \tiny{$\pm$0.4} 	& DGN$(x,x)$ 	& DGN$(x,\mathbf{1})$ 	&  DLGN$(x,x)$ 	& DLGN$(x,\mathbf{1})$ & DSLGN$(x,x)$ 	& DSLGN$(x,\mathbf{1})$\\




%We now present additional theoretical and experimental insights on dual linearity that show that the once the gates are given the learning in the weights is best understood by a path-by-path view and the layer-by-layer view is not relevant.



%Given a DNN, there are two corresponding networks, the LGLN  in \Cref{sec:intro} and the DGN in \Cref{sec:prelim}. The LGLN can be obtained from the DGN by,

%(i) replacing all the ReLU activations in the feature network 

%The DGN performs only marginally poor compared to the DNN, and by successfully addressing the `black box'-ness issue in a DGN we will have both performance and interpretability. We now propose our novel \texttt{DGN-NO-ACT}  which disentangles the computations into (i) primal linear layer-by-layer computation, (ii) dual linear path-by-path computation and (iii) gating non-linearity which \emph{lifts} the computations from the primal to the dual. \texttt{DGN-NO-ACT} (right diagram in \Cref{fig:dgn}) is obtained by making a novel modification to the DGN (left diagram in \Cref{fig:dgn}) as follows:

%1. We replace the ReLUs in the feature network with \emph{identity} maps, i.e., $I(q) = q$. Thus, in the \texttt{DGN-NO-ACT}, the transformation from input to the pre-activations is entirely linear. We call this \textbf{primal linearity}. The disentanglement happens because the ReLU  is completely removed.  

%2. The value network of the DGN is linear in dual `path' variables. We call this \textbf{dual linearity} which stands for the fact that the value network computes path-by-path, and not layer-by-layer. As a result, presenting a $\mathbf{1}$ as input to the value network in \texttt{DGN-NO-ACT} does not degrade performance. The simplifications are : (i) $v_{\Tv}\in \R^{\text{total\,paths}}$ is a vector that does not depend on the input, and (ii) $\phi_{\Tf}(x)\in(0,1)^{\text{total\,paths}}$ is a  simple feature vector. In other words, the value network  learns the paths, and the feature network learns the activations of paths for each input.  While in each layer of the value network GaLUs and the linear operations are entangled, disentanglement happens in the path variables, and we do not have to worry about how learning happens layer-by-layer.

%3. The gates themselves serve the functionality of \emph{lifting} the computations from the primal to the dual, i.e., pre-activations trigger the gates which in turn activate the paths.

%\textbf{Significance.} The salient importance of the \texttt{DGN-NO-ACT} is that it clearly separates the layer-by-layer computations in the feature network and path-by-path computation in the value network. The importance of primal linearity is that we can now use standard linear algebra to `mathematically' interpret the linear transformations from the input to the pre-activations. Also, in the case of specific application domains such as `image classification', these transformations are `filter banks' which have been extensively studied with well known interpretations. The `mathematical' as well as domain specific interpretabilities obviate the need for `locally linear explanation using simpler models': the feature network is entirely linear and is itself simple. The importance of dual linearity is that it gives us a \emph{kernel} based `mathematical' interpretation in the \emph{infinite width regime}. 


%\textbf{DGN-NO-ACT(our model)}(see right diagram in \Cref{fig:dgn}). We replace the ReLUs in the feature network with \emph{identity} maps, i.e., $I(q) = q$. Thus, in the \texttt{DGN-NO-ACT}, the transformation from input to the pre-activations is entirely linear and is amenable to interpertation via standard spectral analysis using linear algebraic tools. The pre-activations trigger the gates, which then dictates the path activity. The value network is given only given $\mathbf{1}$ as input and hence $v_{\Tv}\in \R^{\text{total\,paths}}$ is a vector that does not depend on the input, and $\phi_{\Tf}(x)\in(0,1)^{\text{total\,paths}}$ is a very simple feature vector. % and the output $\hat{y}_{\text{DGN}}(x)=\ip{\phi_{\Tf}(x),v_{\Tv}}=\sum_{p}\phi_\Tf(x,p)v_\Tv(p)$ is equal to \emph{\textbf{the summation of `path value' weighted by the `path activations'}}. 
%Thus, the \texttt{DGN-NO-ACT} can be seen to disentangle the `primal linear' feature network which generates the gates, which in turn activate the paths in the value network which is the `dual linear'.

%\textbf{Remark.} Presenting the value network with $\mathbf{1}$ is very counter intutive, which will get justfied in theory as well as experiments in the next section. As a quick check, we the test accuracies of a DNN $4$ with convolutional layers and global-average-pooling (GAP), its corresponding DGN and \texttt{DGN-NO-ACT} on CIFAR-10 are {\bf{DNN: $80.4\%$,  DGN: $77.4\%$, \texttt{DGN-NO-ACT} : $74.5\%$}}.

%Since $\hat{y}_{\Theta}(x)=\ip{\phi_{x,\Theta},v_{\Theta}}$, during training, as $\Theta$ is learnt, both the NPFs and NPV are also learnt. To understand their roles better, $\phi_{x,\Theta}$ and $v_{\Theta}$ have to be separated.  This is achieved by the deep gated network (DGN) setup (see \Cref{fig:dgn}), which has two networks of \emph{identical architecture} namely the \emph{feature network} ($\Tf\in\R^{d^{\text{f}}_{\text{net}}}$) which holds the NPFs (i.e., the gating information) and the \emph{value network} ($\Tv\in\R^{d^{\text{v}}_{\text{net}}}$) which holds the NPV.  The combined parameterisation is denoted by $\Theta^{\text{DGN}}=(\Tf,\Tv)\in \R^{d^{\text{f}}_{\text{net}}+d^{\text{v}}_{\text{net}}}$.  The feature network is a DNN with ReLUs and the value network is a DNN with \emph{Gated Linear Units (GaLUs)} (terminology used in [\citenum{sss}]) whose output is the product of its pre-activation input $q^{\text{v}}(x)$and the external gating signal $G^{\text{f}}(x)$ (see \Cref{fig:dgn}). In \Cref{fig:dgn}, the main output of the DGN is $\hat{y}_{\text{DGN}}(x)$, while the other output $\hat{y}_{\text{f}}(x)$ is used to \emph{pre-train} the gates (see \Cref{sec:exp}).

