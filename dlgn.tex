\subsection{Numerical Experiments}\label{sec:dlgn}
\begin{comment}
\begin{figure}[!b]
\centering
\begin{minipage}{0.95\columnwidth}
\centering
\resizebox{0.99\columnwidth}{!}{
\input{fig-c4all-folded}
}
\end{minipage}
\caption{\small{Here the gates $G_1,G_2,G_3,G_4$ are generated by the feature network are are permuted as $G_{i_1},G_{i_2},G_{i_3},G_{i_4}$ before applying to the value network. All convolutional layers have $128$ filters each.}}
\label{fig:c4gap}
\end{figure}
\end{comment}
%\begin{comment}
\begin{figure}[!b]
\centering
%\begin{minipage}{0.55\columnwidth}
%\centering
%\resizebox{0.99\columnwidth}{!}{
%\input{fig-c4gap-dnn}
%}
%\end{minipage}
\begin{minipage}{0.11\columnwidth}
\centering
\resizebox{0.99\columnwidth}{!}{
\input{fig-dnn-folded}
}
\end{minipage}
\begin{minipage}{0.40\columnwidth}
\centering
\resizebox{0.99\columnwidth}{!}{
\input{fig-dgn-folded}
}
\end{minipage}
\begin{minipage}{0.40\columnwidth}
\centering
\resizebox{0.99\columnwidth}{!}{
\input{fig-dlgn-folded}
}
\end{minipage}
\centering
\resizebox{.90\columnwidth}{!}{
\begin{tabular}{cccccccc}
\toprule
\multicolumn{8}{c}{Table I}\\
\toprule
Dataset 					& Permute	&C4GAP 				&DGN$(x,x)$ 			&DGN$(x,\mathbf{1})$ 	&DLGN$(x,x)$ 			&DLGN$(x,\mathbf{1})$ 	&$\frac{\text{DLGN}(x,\mathbf{1})}{\text{DNN}}$\\\midrule
\multirow{2}{*}{CIFAR10}		& No			&80.5\tiny{$\pm$0.4} 	&77.4\tiny{$\pm$0.3} 	& 77.5\tiny{$\pm$0.2} 	&75.4\tiny{$\pm$0.3} 	&75.4\tiny{$\pm$0.2}		&$93.66$	\\
						& Yes 		&-- 					&77.3\tiny{$\pm$0.5} 	&77.9\tiny{$\pm$0.6}		&75.9\tiny{$\pm$0.5} 	&76.0\tiny{$\pm$0.5}		&$94.40$	\\\midrule
\multirow{2}{*}{CIFAR100}		& No  		&51.8\tiny{$\pm$0.4} 	&47.4\tiny{$\pm$0.2}		&47.3\tiny{$\pm$0.3} 	&47.4\tiny{$\pm$0.1} 	&48.0\tiny{$\pm$0.2}		&$92.66$\\
						& Yes 		& -- 					&48.4\tiny{$\pm$0.8} 	&49.2\tiny{$\pm$0.9} 	&47.5\tiny{$\pm$1.0} 	&48.4\tiny{$\pm$0.9}		&$93.43$\\
\bottomrule
\end{tabular}
}
\resizebox{0.9\columnwidth}{!}{
\begin{tabular}{cccccccccc}
\toprule
\multicolumn{8}{c}{Table II}\\
\toprule
Dataset 					& Model 		&DNN 				&DGN$(x,x)$ 			&DGN$(x,\mathbf{1})$ 	&DLGN$(x,x)$ 			&DLGN$(x,\mathbf{1})$ 	&$\frac{\text{DLGN}(x,\mathbf{1})}{\text{DNN}}$\\\midrule		
\multirow{2}{*}{CIFAR10}		&VGG16 		&93.6\tiny{$\pm$0.2} 	& 93.0\tiny{$\pm$0.1}  	&93.0\tiny{$\pm$0.1}   	&87.0\tiny{$\pm$0.1}		&87.0\tiny{$\pm$0.2}		&$92.94$	\\
						&ResNet110 	&94.0\tiny{$\pm$0.2} 	& 93.3\tiny{$\pm$0.2} 	&93.2\tiny{$\pm$0.1} 	&87.9\tiny{$\pm$0.2}   	&87.8\tiny{$\pm$0.1} 	&$93.40$	\\\midrule
\multirow{2}{*}{CIFAR100}		&VGG16	 	&73.4\tiny{$\pm$0.3}  	&70.3\tiny{$\pm$0.1} 	&70.5\tiny{$\pm$0.2} 	&61.5\tiny{$\pm$0.2}		&61.5\tiny{$\pm$0.1}		&$\mathbf{83.78}$	\\
						&ResNet110 	&72.7\tiny{$\pm$0.2}		&70.8\tiny{$\pm$0.2} 	&70.8\tiny{$\pm$0.4}		&62.3\tiny{$\pm$0.2} 	&62.7\tiny{$\pm$0.3} 	&$86.24$	\\
\bottomrule
\end{tabular}
}
\caption{\small{Here the gates $G_1, G_2, G_3, G_4$ are generated by the feature network and are permuted as $G_{i_1},G_{i_2},G_{i_3},G_{i_4}$ before applying to the value network. $C_1,C_2,C_3,C_4$ have $128$ filters each. \textbf{Table I and II:} All columns (except the last) show the $\%$ test accuracy on CIFAR-10 and CIFAR-100, and $\%$ of DNN performance recovered by DLGN is in the last column. \textbf{Table I:} For each dataset, the top row has results for vanilla models without permutations (the results are averaged over $5$ runs) and the bottom row has results of $4!-1=23$ permutations (except the identity) for each model (the results are averaged over the $23$ permutations). }}
\label{fig:c4gap}
\end{figure}
%\end{comment}
\Cref{sec:analysis}  presented theoretical results which demystified the layer-by-layer view in value network, in this section we will verify these theoretical results in experiments. We then show that DLGN recovers major part of performance of state-of-the-art DNNs on CIFAR-10 and CIFAR-100.

%$\bullet$ Claim I: \emph{The weight/value network is disentangled in the path space}. 

%$\bullet$ Claim II: \emph{Disentangled computations in a DLGN recovers most performance of DNNs.}

\textbf{Setup Details.} We consider $3$ DNN architectures, C4GAP, VGG-16 and Resnet-110, and their DGN and DLGN counterparts. Here C4GAP is a simple model (achieves about $80\%$ accuracy on CIFAR-10), mainly used to verify the theoretical insights in \Cref{sec:analysis}. VGG-16 and Resnet-110 are chosen for their state-of-the-art performance on CIFAR-10 and CIFAR-100. All models are trained using off-the-shelf optimisers (for more details, see \Cref{sec:expdetails}). The DGN and DLGN are trained from scratch, i.e., both the feature and value network are initialised at random and trained. In DGN and DLGN, we use soft gating (see \Cref{fig:dgn}) so that gradient flows through the feature network and the gates are learnt (we chose $\beta=10$).  In what follows, we use the notation DGN$(\xf,\xv)$ and DLGN$(\xf,\xv)$ where $\xf$ and $\xv$ denote the input to the value and feature networks respectively. For instance, DGN$(x,x)$ will mean that both the value and feature network of the DGN is provided the image as input, and DLGN$(x,\mathbf{1})$ will mean that the feature network is given with the image as input and the value network is given a constant $\mathbf{1}$ as input. 

\textbf{Disentangling Value Network.} We show that destroying the layer-by-layer structure via permutations and providing a constant $\mathbf{1}$ input do not degrade performance. Since our aim here is not state-of-the-art performance, we use C4GAP with $4$ convolutional layers which achieves only about $80\%$ test accuracy on CIFAR-10, however, enables us to run all the $4!=24$ layer permutations. The C4GAP, DGN and DLGN with layer permutations are shown in \Cref{fig:c4gap}. Once a permutation is chosen, it is fixed during both training and testing. The results in Table I of \Cref{fig:c4gap} show that there is no significant difference in performance between DGN$(x,x)$ vs DGN$(x,\mathbf{1})$, and DLGN$(x,x)$ vs DLGN$(x,\mathbf{1})$, i.e., constant $\mathbf{1}$ input does not hurt. Also, there is no significant difference between the models without permutations and the models with permutations.  These counter intuitive and surprising results are difficult to explain using the commonly held `sophisticated features are learnt layer-by-layer' view. However, neither the permutations or the constant $\mathbf{1}$ input destroys the correlation in the gates, and are not expected to degrade performance as per the insights in \Cref{sec:analysis}. This verifies Claim I.  %Similar results on CIFAR-100 is shown in the Appendix.
\begin{comment}
\begin{table}[!t]
\centering
\resizebox{1.0\columnwidth}{!}{
\begin{tabular}{cccccccccc}
\toprule
\multicolumn{8}{c}{Table II}\\
\toprule
Dataset 					& Model 		&DNN 				&DGN$(x,x)$ 			&DGN$(x,\mathbf{1})$ 	&DLGN$(x,x)$ 			&DLGN$(x,\mathbf{1})$ 	&$\frac{\text{DLGN}(x,\mathbf{1})}{\text{DNN}}$\\\midrule		
\multirow{2}{*}{CIFAR10}		&VGG16 		&93.6\tiny{$\pm$0.2} 	& 93.0\tiny{$\pm$0.1}  	&93.0\tiny{$\pm$0.1}   	&87.0\tiny{$\pm$0.1}		&87.0\tiny{$\pm$0.2}		&$92.94$	\\
						&ResNet110 	&94.0\tiny{$\pm$0.2} 	& 93.3\tiny{$\pm$0.2} 	&93.2\tiny{$\pm$0.1} 	&87.9\tiny{$\pm$0.2}   	&87.8\tiny{$\pm$0.1} 	&$93.40$	\\\midrule
\multirow{2}{*}{CIFAR100}		&VGG16	 	&73.4\tiny{$\pm$0.3}  	&70.3\tiny{$\pm$0.1} 	&70.5\tiny{$\pm$0.2} 	&61.5\tiny{$\pm$0.2}		&61.5\tiny{$\pm$0.1}		&$\mathbf{83.78}$	\\
						&ResNet110 	&72.7\tiny{$\pm$0.2}		&70.8\tiny{$\pm$0.2} 	&70.8\tiny{$\pm$0.4}		&62.3\tiny{$\pm$0.2} 	&62.7\tiny{$\pm$0.3} 	&$86.24$	\\
\bottomrule
\end{tabular}
}
\caption{\small{All columns (except the last) show the $\%$ test accuracy on CIFAR-10 and CIFAR-100. The results are averaged over $5$ runs. The last column shows the $\%$ of DNN performance that the DLGN recovers.}}
\label{tb:expresults}
\end{table}
\end{comment}

%In this section, we show experimental results comparing performance of standard DNNs with their DGN and DLGN counterparts on CIFAR-10 and CIFAR-100. 
% %VGG-16 and ResNet-110 are chosen for their state of the art performance on standard datasets namely CIFAR-10 and CIFAR-100. 

%\textbf{Training.} In DGN and DLGN, we use soft gating (see \Cref{fig:dgn}) so that gradient flows through the feature network and the gates are learnt (we chose $\beta=10$). For experiments in \Cref{tb:permute}, we used \emph{Adam} optimiser, with learning rate of $3\times 10^{-4}$ and batch size of $32$. For  experiments in \Cref{tb:permute}, we used  \emph{SGD} optimiser with momentum of $0.9$ and trained for $64000$ steps with batch size of $128$ per step. The learning rate schedule was $0.01$ for first $400$ steps, and $0.1$ for steps $401$ to $3200$ and $0.01$ for steps $32001$ to $48000$ and $0.001$ for steps $48001$ to $64000$.


\textbf{DLGN Performance.} For this we choose VGG-16 and Resnet-110. The results in Table II of \Cref{fig:c4gap} show that the DLGN recovers more than $83.5\%$ (i.e., $83.78\%$ in the worst case) of the performance of the state-of-the-art DNN. %This verifies Claim II.
While entanglement in the DNNs enable their improved performance,  the `disentangled and interpretable'  computations in the DLGN recovers most part of the performance. %This result also implies that the most important function of gating is to \emph{lift} the linear computations in the feature network to the dual linear computations in the value network.

\subsection{Is DLGN a Universal Spectral Approximator}
Motivated by the success of DLGN, we further break it down into DLGN-\emph{Shallow Features} (DLGN-SF), wherein, the feature network is a collection of shallow single matrix multiplications. We compare a shallow DNN with ReLU called C1GAP against DLGN-SF of C4GAP (see \Cref{fig:shallow}) and VGG-16 (see Appendix). The results are in \Cref{fig:shallow}, based on which we observe the following:

$\bullet$ \textbf{Power of depth in value network and lifting to dual space.} Both C1GAP and C4GAP-DLGN-SF were trained with identical batch size, optimiser and learning rate (chosen to be the best for C1GAP). The performance of C1GAP at $200$ epochs is $\sim10\%$ lower than that of C4GAP-DLGN-SF. After $2000$ epochs of training and ensembling 16 such C1GAPs as C1GAP-\texttt{16-ENS} closes the gap within $\sim 3\%$ on C4GAP-DLGN-SF. Yet, a deeper architecture VGG-16-DLGN-SF is $\sim10\%$ better than C1GAP-\texttt{16-ENS}.  Note that both VGG-16-DLGN-SF and C1GAP-\texttt{16-ENS} have gates for $16$ layers produced in a shallow manner. While in a C1GAP-\texttt{16-ENS}, `16' C1GAPs are ensembled, in VGG-16-DLGN-SF these gates for 16 layers are used as gating signals to turn `on/off' the GaLUs laid depth-wise as 16 layers of the value network, which helps to lift the computations to the dual space. Thus, using the gates to \textbf{lift} (instead of ensembling) the computations to the dual space in the value network is playing a critical role, investigating which is an important future work.%(lifting is caused only if the gates laid out depth-wise as in the value netwrok as opposed to ensemble of 16 models in C1GAP-\texttt{16-ENS}).

$\bullet$ \textbf{Power of depth in feature network.} By comparing CIFAR-100 performance of VGG-16-DLGN-SF in \Cref{fig:shallow} and that of VGG-16-DLGN in \Cref{fig:c4gap}, we see $\sim 6\%$ improvement if we have a deep linear network instead of many shallow linear networks as the feature network. This implies depth helps even if the feature network is entirely linear, investigating which is an important future work. 

$\bullet$ \textbf{DGN vs DLGN} In Table I and II of \Cref{fig:c4gap}, the difference between DGN and DNN is minimal (about $3\%$), however, the difference between DLGN and DNN is significantly large. Thus, it is important to understand the role of the ReLUs in the feature network of DGN. It is interesting to know whether this is simpler than understanding the DNN with ReLUs itself.

\begin{figure}
\centering
\resizebox{0.9\columnwidth}{!}{
\input{fig-dlgn-sl-folded}
}
\resizebox{0.9\columnwidth}{!}{
\begin{tabular}{ccccccc}
\toprule
\multicolumn{7}{c}{Table III}\\
\toprule
Dataset 			&C1GAP 				&C1GAP-\texttt{16-ENS}		&C1GAP 				&C1GAP-\texttt{16-ENS}		&C4GAP-DLGN-SF	&VGG-16-DLGN-SF\\
\texttt{(epochs)} 		&\texttt{(200)}	 		&\texttt{(200)}			&\texttt{(2000)} 		&\texttt{(2000)}				&\texttt{(50)}					&\texttt{(32)}\\\midrule		
CIFAR-10			&62.2\tiny{$\pm$0.2}		&63.81					&70.0\tiny{$\pm$0.2}		&72.2					&75.1\tiny{$\pm$0.4}				&84.9\tiny{$\pm$0.2}\\ 
CIFAR-100		&36.3\tiny{$\pm$0.2}		&38.23					&42.2\tiny{$\pm$0.3}		&45.47					&47.3\tiny{$\pm$0.6}				&56.3\tiny{$\pm$0.2}\\
\bottomrule
\end{tabular}
}
\caption{\small{C1GAP and C4GAP have width = 512 to make them comparable to VGG-16 whose maximum width is 512. The ensemble size is 16 to match the 16 layers of VGG-16. Note that C4GAP in \Cref{fig:c4gap} has width=128. The last two columns show only DLGN-SF$(x,\mathbf{1})$. We observed the performance of DLGN-SF$(x,x)$ to be $\sim 2\%$ lesser and left it out from Table III for sake brevity. All results except for \texttt{ENS} are averaged over $5$ runs.}}
\label{fig:shallow}
\end{figure}

$\bullet$ \textbf{Is DLGN a Universal Spectral Approximator?} The value network realises the NPK which in general is an ensemble (assuming skip connections). The NPK is based on the gates whose pre-activations are generated linearly. It is interesting to ask whether the DLGN via its feature network learns the right linear transformations to extract the relevant spectral features (to tigger the gates) and via its value network learns  the ensembling of kernels (based on gates) in a dataset dependent manner.

%To elaborate, any given ReLU in a layer gets triggered if the input to that layer is in the positive half-space corresponding to the hyperplane dictated by the incoming weights of the given ReLU. 
%Consider the domain of image classification. We know that the feature network in a DLGN transforms the input using a sequence of `filter banks', and hence it is possible to use standard tools from `image processing' and `filter banks' theory to understand the meaning of the transformed images in the feature network. Note that if there are non-linearities present then it is not clear as to whether the hidden layer outputs are interpretable as images. 

 %Note that all these questions are related to the feature network, because, we have shown that the role of the gates and the value network is to obtain the gating features and realise the corresponding NPK.



%Recall that in a DLGN, the gating/feature network is a deep linear network without non-linearities, and hence is disentangled by construction. We claimed in \Cref{sec:intro} that the weight/value network is \emph{disentangled in the path space}, for which, as promised we presented theoretical insights in \Cref{sec:analysis}, and now we will provide the supporting experimental evidence. 



%will experimentally compared DNNs with their DGN and DLGN counterparts, to show how the `disentangled and interpretable' computations in a DLGN recover major part of the performance of DNNs. Recall that the gating/


%This also implies that while non-linearity in ReLU causes entanglement (and hence non-interpretability), the major function of the ReLU as gates is to lift the layer-by-layer computations in the gating/feature network to the path-by-path computations in the weight/value network. 
 

%In this section, we turn to the issues of entanglement and `black box'-ness. We start by noting that the performance in the \emph{learnable gates} setting of the DGN show us that the DGN is not only conceptually similar to the DNN, but also empirically approximate to the DNN. In this paper, we modify the DGN to obtain the LGLN. In particular, the LGLN (\Cref{fig:lgln}) is same as the DGN in which (i) the ReLUs in the feature network are replaced with identity activations, and (ii) the input to the value network is $\mathbf{1}$ (see bottom network of LGLN in \Cref{fig:lgln} and the input to the value network of the DGN in \Cref{fig:dgn}). We now discuss how the LGLN disentangles the computations into two linear structures to show that the commonly held view of sophisticated structures are learnt in a layer-by-layer manner is misconceived.  From now on, as it is with the DGN, we will refer to the top part of the LGLN as its feature network and the bottom part as its value network. 



%Continuing with the `learning in gates + learning in the weights given gates' interpretation, using LGLN (\Cref{fig:lgln}), we disentangle both `learning in the gates' as well as `learning in the weights given gates'. 


%We now elaborate on why these changes are needed in the LGLN and how disentangling is achieved.

%$\bullet$ \textbf{Disentangling Feature Network.}  %The gates themselves encode binary information and are triggered by their pre-activations.In a DGN, the transformation from the input to the pre-activation to the gates happens through the hidden layers, in which the linear and non-linear operations are entangled. The hidden layers are not interpretable, and we have to fall back to sophisticated structures being learnt in the hidden layers as an explanation for why it is possible to learn useful pre-activations (as hence useful gates). In LGLN, the non-linear ReLUs are removed, and the transformation in linear, i.e., while the learning of pre-activations happens in a layer-by-layer manner, the learnt structures are no longer sophisticated and are interpretable in terms of standard linear algebra. We will call this \textbf{primal linearity}. Primal linearity is ensured via construction itself and needs no further theoretical justification, and in what follows, we experimentally verify in \Cref{sec:exp} that linearly learnt pre-activations does not degrade performance significantly, when compared to state of the art.

%$\bullet$ \textbf{Disentangling Value Network.} %Note that the value network of the LGLN and that of the DGN are the same, except that in the DGN the value network is given $x\in\R^{\din}$ as the input and in the LGLN it is a constant $\mathbf{1}\in\R^{\din}$ instead.  We show via experiments in \Cref{sec:exp} that destroying the layer-by-layer structure and the constant $\mathbf{1}$ does not degrade performance. These results are counter intuitive, surprising, and more importantly difficult to reconcile with using the `sophisticated features are learnt in a layer-by-layer' explanation. However, we argue that these surprising results can be readily explained using the NPK expression in \Cref{th:fcprev}.  Note that in a DGN, learning the weights with fixed gates amounts to learning a linear model in the dual variables, i.e., learning $\hat{y}_{\text{DGN}}(x)=\ip{\phi_{\Tf}(x), v_{\Tv}}$. We called this \textbf{dual linearity} and in \Cref{sec:prelim} showed that dual linearity has a kernel interpretation in terms of the NPK in \Cref{th:fcprev}. 
%Thus, giving a constant $\mathbf{1}$ input would mean that $\ip{x,x'}=\din$, and the expression on the right reduces to $d \cdot \sigma^{2(d-1)}\cdot \din\cdot \textbf{overlap}(x,x')$. Now a previously unnoticed fact is that $\textbf{overlap}(x,x')$ is invariant to layer permutations, which implies destroying the layer-by-layer structure does not hurt. In short, the main role of the gating non-linearity is to lift the computations from the primal to the dual space. In the dual space, the right way to interpret the computations is path-by-path and the question of sophisticated structures being learnt in the layers is irrelevant.
%However, in terms of the dual variables, the constant $\mathbf{1}$ has the following interpretation: the neural path value is a vector specifying the contribution of each path to the final output and the neural path feature is a vector specifying the path activity (from \Cref{def:npf-npv} $\phi(x,p)=x\cdot A(x,p)$, and since input is $1$, we have $\phi(x,p)=A(x,p)$), i.e., which path to be selected and which one to be left out in the output. The destruction of layers is possible due to the permutation invariance property of the NPK itself which was unnoticed in prior work. 

%In what follows, in \Cref{th:fc}, we first restate Theorem 5.1 in \cite{npk} (i.e., \Cref{th:fcprev}) so as to explicitly capture the permutation invariance. In order to build a  more complete picture, we extend the dual view to cover the cases of convolution with global pooling and skip connections. We then present the aforementioned experimental results in \Cref{sec:exp}.

%What happens when we give $\mathbf{1}$ input to value network?

%What happens when we give permute the gates?

%Interpreting DLGN$(x,\mathbf{1})$

%Interpreting DSLGN$(x,\mathbf{1})$

%Power of Depth Alone


\begin{comment}
\begin{figure}
\centering
\begin{minipage}{1.0\columnwidth}
\centering
\resizebox{0.99\columnwidth}{!}{
\input{fig-c4gap-permute}
}
\resizebox{0.99\columnwidth}{!}{
\input{fig-dlgn-sl-folded}
}
\end{minipage}
\caption{$4$ convolutional layers with GAP}
\label{fig:c4gap}
\end{figure}




\begin{figure}
\centering
\begin{minipage}{1.0\columnwidth}
\centering
\begin{minipage}{0.49\columnwidth}
\resizebox{0.99\columnwidth}{!}{
\input{fig-c4dgngap-permute}
}
\end{minipage}
\begin{minipage}{0.49\columnwidth}
\resizebox{0.99\columnwidth}{!}{
\input{fig-c4lglngap-permute}
}
\end{minipage}
\end{minipage}
\caption{$4$ convolutional layers with GAP}
\label{fig:c4gap}
\end{figure}
\end{comment}

\begin{comment}
\begin{table}[!t]
\centering
\resizebox{1.0\columnwidth}{!}{
\begin{tabular}{cccccccc}
\toprule 
Dataset 					& Permute	&CNN-GAP 				&DGN$(x,x)$ 			&DGN$(x,\mathbf{1})$ 	&DLGN$(x,x)$ 			&DLGN$(x,\mathbf{1})$ 	&$\frac{\text{DLGN}(x,\mathbf{1})}{\text{DNN}}$\\\midrule
\multirow{2}{*}{CIFAR10}		& No			&80.5\tiny{$\pm$0.4} 	&77.4\tiny{$\pm$0.3} 	& 77.5\tiny{$\pm$0.2} 	&75.4\tiny{$\pm$0.3} 	&75.4\tiny{$\pm$0.2}		&$93.66$	\\
						& Yes 		&-- 					&77.3\tiny{$\pm$0.5} 	&77.9\tiny{$\pm$0.6}		&75.9\tiny{$\pm$0.5} 	&76.0\tiny{$\pm$0.5}		&$94.40$	\\\midrule
\multirow{2}{*}{CIFAR100}		& No  		&51.8\tiny{$\pm$0.4} 	&47.4\tiny{$\pm$0.2}		&47.3\tiny{$\pm$0.3} 	&47.4\tiny{$\pm$0.1} 	&48.0\tiny{$\pm$0.2}		&$92.66$\\
						& Yes 		& -- 					&48.4\tiny{$\pm$0.8} 	&49.2\tiny{$\pm$0.9} 	&47.5\tiny{$\pm$1.0} 	&48.4\tiny{$\pm$0.9}		&$93.43$\\
\bottomrule
\end{tabular}
}
\caption{\small{All columns (except the last) show the $\%$ test accuracy on CIFAR-10 and CIFAR-100. For each dataset, the top row has results for vanilla models without permutations; the results are averaged over $5$ runs. For each dataset, the bottom row has results of $4!-1=23$ permutations (except the identity) for each model; the results are averaged over the $23$ permutations. The $\%$ of DNN performance recovered by DLGN is in the last column.}} %has the $\%$ of DNN performance that the DLGN recovers.}}
\label{tb:permute}
\end{table}
\end{comment}

\begin{comment}
\begin{table}[!t]
\centering
\resizebox{1.0\columnwidth}{!}{
\begin{tabular}{ccccccc}
\toprule 
Dataset 			&C1GAP 				&C1GAP-\texttt{16-ENS}		&C1GAP 				&C1GAP-\texttt{16-ENS}		&C4GAP-DLGN$(x,\mathbf{1})$	&VGG-16-DLGN$(x,\mathbf{1})$\\
\texttt{(epochs)} 		&\texttt{(200)}	 		&\texttt{(200)}				&\texttt{(2000)} 			&\texttt{(2000)}				&\texttt{(50)}					&\texttt{(32)}\\\midrule		
CIFAR-10			&62.2\tiny{$\pm$0.2}		&63.81					&70.0\tiny{$\pm$0.2}		&72.2					&75.1\tiny{$\pm$0.4}				&84.9\tiny{$\pm$0.2}\\ 
CIFAR-100		&36.3\tiny{$\pm$0.2}		&38.23					&42.2\tiny{$\pm$0.3}		&45.47					&47.3\tiny{$\pm$0.6}				&56.3\tiny{$\pm$0.2}\\
\bottomrule
\end{tabular}
}
\caption{\small{}}
\label{tb:shallow}
\end{table}
\end{comment}


\begin{comment}
\begin{table}[!t]
\centering
\resizebox{1.0\columnwidth}{!}{
\begin{tabular}{ccccccccc}
\toprule 
Dataset 					& Model 				&DNN 				&DGN$(x,x)$ 			&DGN$(x,\mathbf{1})$ 	&DLGN$(x,x)$ 			&DLGN$(x,\mathbf{1})$ 	&DSLGN$(x,x)$ 		&DSLGN$(x,\mathbf{1})$  \\\midrule
\multirow{2}{*}{CIFAR10}		& C4GAP 				&80.5\tiny{$\pm$0.4} 	&77.4\tiny{$\pm$0.3} 	& 77.5\tiny{$\pm$0.2} 	&75.4\tiny{$\pm$0.3} 	&75.4\tiny{$\pm$0.2}		&74.2\tiny{$\pm$0.2}	 	&73.3\tiny{$\pm$0.2} \\
						& C4GAP-PERMUTE 	&-- 					&77.3\tiny{$\pm$0.5} 	&77.9\tiny{$\pm$0.6}		&75.9\tiny{$\pm$0.5} 	&76.0\tiny{$\pm$0.5}		& -- 					& -- \\\midrule
\multirow{2}{*}{CIFAR100}		& C4GAP 				&51.8\tiny{$\pm$0.4} 	&47.4\tiny{$\pm$0.2}		&47.3\tiny{$\pm$0.3} 	&47.4\tiny{$\pm$0.1} 	&48.0\tiny{$\pm$0.2}		&45.8\tiny{$\pm$0.3}	 	&44.9\tiny{$\pm$0.1}	\\
						& C4GAP-PERMUTE 	& -- 					&48.4\tiny{$\pm$0.8} 	&49.2\tiny{$\pm$0.9} 	&47.5\tiny{$\pm$1.0} 	&48.4\tiny{$\pm$0.9}		& --					& -- \\
\bottomrule
\end{tabular}
}
\caption{Summary of Experiments}
\label{tb:premresults}
\end{table}

\begin{table}[!t]
\centering
\resizebox{1.0\columnwidth}{!}{
\begin{tabular}{ccccccccc}
\toprule 
Dataset 					& Model 		&DNN 				&DGN$(x,x)$ 			&DGN$(x,\mathbf{1})$ 	&DLGN$(x,x)$ 			&DLGN$(x,\mathbf{1})$ 	&DSLGN$(x,x)$ 		&DSLGN$(x,\mathbf{1})$  \\\midrule		
\multirow{2}{*}{CIFAR10}		&VGG16-AP 	&93.6\tiny{$\pm$0.2} 	& 93.0\tiny{$\pm$0.1}  	&93.0\tiny{$\pm$0.1}   	&87.0\tiny{$\pm$0.1}		&87.0\tiny{$\pm$0.2}		&84.8\tiny{$\pm$0.3} 	&84.9\tiny{$\pm$0.2} \\
						&ResNet110 	&94.0\tiny{$\pm$0.2} 	& 93.3\tiny{$\pm$0.2} 	&93.2\tiny{$\pm$0.1} 	&87.9\tiny{$\pm$0.2}   	&87.8\tiny{$\pm$0.1} 	&72.4{$\pm$13.8} 		&71.2{$\pm$17.3}  \\\midrule
\multirow{2}{*}{CIFAR100}		&VGG16-AP 	&73.4\tiny{$\pm$0.3}  	&70.3\tiny{$\pm$0.1} 	&70.5\tiny{$\pm$0.2} 	&61.5\tiny{$\pm$0.2}		&61.5\tiny{$\pm$0.1}		&56.4\tiny{$\pm$0.4} 	&56.3\tiny{$\pm$0.2}\\
						&ResNet110 	&72.7\tiny{$\pm$0.2}		&70.8\tiny{$\pm$0.2} 	&70.8\tiny{$\pm$0.4}		&62.3\tiny{$\pm$0.2} 	&62.7\tiny{$\pm$0.3} 	&49.8{$\pm$0.9}		&41.5{$\pm$15.5}  \\
\bottomrule
\end{tabular}
}
\caption{Summary of Experiments}
\label{tb:expresults}
\end{table}
\end{comment}





\begin{comment}
\subsection{Open Question: Upstream training with random labels}\label{sec:exp2}
In this section, we add more evidence in favour of why the dual view is a useful when it comes to disentangling and understanding the inner workings of DNNs with ReLUs. For this, we turn to the study of \cite{randlabel} who studied both positive and negative effects on downstream training performance due to upstream training with random labels. However, the following question on test performance is open: \emph{When trained with random labels upstream followed by true labels downstream, the test performance of a DNN with ReLUs degrades, Why?}

\textbf{Answer Using Dual View}. Let us denote training by true labels by subscript $\text{T}$ and training by random labels first followed by true labels by subscript RT. In what follows, we also drop $\Theta$ from the notation. When trained with true labels, the DNN learns the relation $\hat{y}(x)=\ip{\phi_{\text{T}}(x),v_{\text{T}}}$. The scenario in the open question corresponds to learning  $\hat{y}(x)=\ip{\phi_{\text{RT}}(x),v_{\text{RT}}}$. In order to disentangle the effect of the random labels on the gates and the weights separately, we consider the following two scenarios: (i) learning $\hat{y}(x)=\ip{\phi_{\text{RT}}(x),v_{\text{T}}}$; this can be achieved using the DGN setup by pre-training the feature network first with random labels followed by true labels, and then training the value network with true labels. (ii) learning $\hat{y}(x)=\ip{\phi_{\text{T}}(x),v_{\text{RT}}}$; this can be achieved using the DGN setup by pre-training the feature network with true labels, and then training the value network first with random labels followed by true labels.
\begin{table}[!t]
\centering
%\resizebox{1.0\columnwidth}{!}{
\begin{tabular}{cccc}
\toprule 
$\hat{y}(x)=\ip{\phi_{\text{T}}(x),v_{\text{T}}}$ 	&$\hat{y}(x)=\ip{\phi_{\text{T}}(x),v_{\text{RT}}}$ 	&$\hat{y}(x)=\ip{\phi_{\text{RT}}(x),v_{\text{RT}}}$ 	&$\hat{y}(x)=\ip{\phi_{\text{RT}}(x),v_{\text{T}}}$ 	\\\midrule
80.3\tiny{$\pm$0.2}						&78.2\tiny{$\pm$0.3}							&68.5\tiny{$\pm$0.3}							&69.5\tiny{$\pm$0.2}								\\
\bottomrule
\end{tabular}
%}
\caption{\small{Random labels affect the gates. In column $2$ the random labels were corrupted by $100\%$. In columns $3$ and $4$ the random labels were corrupted by $75\%$, because we could not train the DNN with $100\%$ corrupt labels.}}
\label{tb:randlabel}
\end{table}

We chose the $4$-layer CNN-GAP and and its corresponding DGN to obtain the results in \Cref{tb:randlabel}. It is evident from column $2$ that if the gates, i.e., the neural path features are \emph{clean}, training the weights, i.e., the neural path value with random labels first followed by true labels degrades test performance only by $3\%$. Also, it is evident from column $4$ that if the gates, i.e., the neural path features are \emph{corrupt}, training the weights, i.e., the neural path values with true labels still have poor performance by about $11.5\%$. Based on these, we conclude that, \emph{degradation in the test performance due to upstream training with random labels is  because the gates, i..e, neural path features degrade.}
\end{comment}
%& C4GAP 		& 51.8 \tiny{$\pm$0.4} 	& DGN$(x,x)$ 	& DGN$(x,\mathbf{1})$ 	&  DLGN$(x,x)$ 	& DLGN$(x,\mathbf{1})$ & DSLGN$(x,x)$ 	& DSLGN$(x,\mathbf{1})$\\




%We now present additional theoretical and experimental insights on dual linearity that show that the once the gates are given the learning in the weights is best understood by a path-by-path view and the layer-by-layer view is not relevant.



%Given a DNN, there are two corresponding networks, the LGLN  in \Cref{sec:intro} and the DGN in \Cref{sec:prelim}. The LGLN can be obtained from the DGN by,

%(i) replacing all the ReLU activations in the feature network 

%The DGN performs only marginally poor compared to the DNN, and by successfully addressing the `black box'-ness issue in a DGN we will have both performance and interpretability. We now propose our novel \texttt{DGN-NO-ACT}  which disentangles the computations into (i) primal linear layer-by-layer computation, (ii) dual linear path-by-path computation and (iii) gating non-linearity which \emph{lifts} the computations from the primal to the dual. \texttt{DGN-NO-ACT} (right diagram in \Cref{fig:dgn}) is obtained by making a novel modification to the DGN (left diagram in \Cref{fig:dgn}) as follows:

%1. We replace the ReLUs in the feature network with \emph{identity} maps, i.e., $I(q) = q$. Thus, in the \texttt{DGN-NO-ACT}, the transformation from input to the pre-activations is entirely linear. We call this \textbf{primal linearity}. The disentanglement happens because the ReLU  is completely removed.  

%2. The value network of the DGN is linear in dual `path' variables. We call this \textbf{dual linearity} which stands for the fact that the value network computes path-by-path, and not layer-by-layer. As a result, presenting a $\mathbf{1}$ as input to the value network in \texttt{DGN-NO-ACT} does not degrade performance. The simplifications are : (i) $v_{\Tv}\in \R^{\text{total\,paths}}$ is a vector that does not depend on the input, and (ii) $\phi_{\Tf}(x)\in(0,1)^{\text{total\,paths}}$ is a  simple feature vector. In other words, the value network  learns the paths, and the feature network learns the activations of paths for each input.  While in each layer of the value network GaLUs and the linear operations are entangled, disentanglement happens in the path variables, and we do not have to worry about how learning happens layer-by-layer.

%3. The gates themselves serve the functionality of \emph{lifting} the computations from the primal to the dual, i.e., pre-activations trigger the gates which in turn activate the paths.

%\textbf{Significance.} The salient importance of the \texttt{DGN-NO-ACT} is that it clearly separates the layer-by-layer computations in the feature network and path-by-path computation in the value network. The importance of primal linearity is that we can now use standard linear algebra to `mathematically' interpret the linear transformations from the input to the pre-activations. Also, in the case of specific application domains such as `image classification', these transformations are `filter banks' which have been extensively studied with well known interpretations. The `mathematical' as well as domain specific interpretabilities obviate the need for `locally linear explanation using simpler models': the feature network is entirely linear and is itself simple. The importance of dual linearity is that it gives us a \emph{kernel} based `mathematical' interpretation in the \emph{infinite width regime}. 


%\textbf{DGN-NO-ACT(our model)}(see right diagram in \Cref{fig:dgn}). We replace the ReLUs in the feature network with \emph{identity} maps, i.e., $I(q) = q$. Thus, in the \texttt{DGN-NO-ACT}, the transformation from input to the pre-activations is entirely linear and is amenable to interpertation via standard spectral analysis using linear algebraic tools. The pre-activations trigger the gates, which then dictates the path activity. The value network is given only given $\mathbf{1}$ as input and hence $v_{\Tv}\in \R^{\text{total\,paths}}$ is a vector that does not depend on the input, and $\phi_{\Tf}(x)\in(0,1)^{\text{total\,paths}}$ is a very simple feature vector. % and the output $\hat{y}_{\text{DGN}}(x)=\ip{\phi_{\Tf}(x),v_{\Tv}}=\sum_{p}\phi_\Tf(x,p)v_\Tv(p)$ is equal to \emph{\textbf{the summation of `path value' weighted by the `path activations'}}. 
%Thus, the \texttt{DGN-NO-ACT} can be seen to disentangle the `primal linear' feature network which generates the gates, which in turn activate the paths in the value network which is the `dual linear'.

%\textbf{Remark.} Presenting the value network with $\mathbf{1}$ is very counter intutive, which will get justfied in theory as well as experiments in the next section. As a quick check, we the test accuracies of a DNN $4$ with convolutional layers and global-average-pooling (GAP), its corresponding DGN and \texttt{DGN-NO-ACT} on CIFAR-10 are {\bf{DNN: $80.4\%$,  DGN: $77.4\%$, \texttt{DGN-NO-ACT} : $74.5\%$}}.

%Since $\hat{y}_{\Theta}(x)=\ip{\phi_{x,\Theta},v_{\Theta}}$, during training, as $\Theta$ is learnt, both the NPFs and NPV are also learnt. To understand their roles better, $\phi_{x,\Theta}$ and $v_{\Theta}$ have to be separated.  This is achieved by the deep gated network (DGN) setup (see \Cref{fig:dgn}), which has two networks of \emph{identical architecture} namely the \emph{feature network} ($\Tf\in\R^{d^{\text{f}}_{\text{net}}}$) which holds the NPFs (i.e., the gating information) and the \emph{value network} ($\Tv\in\R^{d^{\text{v}}_{\text{net}}}$) which holds the NPV.  The combined parameterisation is denoted by $\Theta^{\text{DGN}}=(\Tf,\Tv)\in \R^{d^{\text{f}}_{\text{net}}+d^{\text{v}}_{\text{net}}}$.  The feature network is a DNN with ReLUs and the value network is a DNN with \emph{Gated Linear Units (GaLUs)} (terminology used in [\citenum{sss}]) whose output is the product of its pre-activation input $q^{\text{v}}(x)$and the external gating signal $G^{\text{f}}(x)$ (see \Cref{fig:dgn}). In \Cref{fig:dgn}, the main output of the DGN is $\hat{y}_{\text{DGN}}(x)$, while the other output $\hat{y}_{\text{f}}(x)$ is used to \emph{pre-train} the gates (see \Cref{sec:exp}).

