\subsection{Primal: Layer-By-Layer Computation in Feature Network}\label{sec:primal}
In a DNN ($\hat{y}(x)=\ip{\phi_\Theta(x),v_{\Theta}}$ as well as a DGN ($\hat{y}_{\text{DGN}}(x)=\ip{\phi_\Tf(x),v_{\Tv}}$, the neural path feature i.e., $\phi(x)$ is learnt during training and it was shown in \citep{npk} that such learning improves generalisation. The $\phi(x)$ is dictated by the input to the network and the gates. 
In this paper, we will not be theoretically analysing how the feature network learns the parameters so as to learn aid generalisation. Instead, we will now discuss how in comparison to a DNN or DGN, the absence of non-linear activations in a \texttt{DGN-NO-ACT} simplifies the interpretation of the neural path feature. 

In DNN, DGN and \texttt{DGN-NO-ACT}, the relationship between the layer input and the pre-activation that triggers a gate in a given layer is simple: it is based on the inner product between the layer input and the weight vector of the incoming weights to that particular gate. In order to understand how the neural path features are generated, we need to have the entire gating pattern, which implies, we have to understand how the input itself gets processed in a layer-by-layer manner. As mentioned earlier, in the case of DNN and DGN, we are hit by the same roadblock, which is, the entangling of the linear matrix multiplication and the non-linear activations. Thus it is not very straightfoward to comment about how the input gets transformed as it gets processed layer-by-layer. 


In a \texttt{DGN-NO-ACT} the feature network is fully linear, which means, one can directly inspect the matrices that are learnt. This in itself is a huge gain, because, we do not have to resort to  `locally linear explainations'; the feature network itself is linear. It will be an interesting empirical exercise to examine the kind of matrices learnt post training; we reserve this study for future work. However, the linearity gives us more \emph{apriori} insights, which we discuss below. 

%Let us keep image classification as an application domain. Let us suppose the feature network has only one fixed filter corresponding to matrix $M$ in all its layers. Then, for input $x$, the output of the layers are $Mx, M^2x, M^3x,\ldots M^{d-1}x$. The 'linear algebraic' interpretation of $M^i x$ ($i=1,\ldots,d-1$) is by looking at the eigenspace of $M$. In this interpretation, the role of depth is to implement 'power method' wherein as depth increases $M^d x$ aligns to the dominant eigenvector of $M$. Let us now consider the case in which the feature network has two fixed filters say $M_1$ and $M_2$ in all its layers. Then, for input $x$, the outputs of the layer are, Layer-1: $M_1x$ and  $M_2x$, Layer-2: $M^2_1x\,+\,M_1M_2x$ and $M_2M_1x \,+\, M^2_2x$, Layer-3:  $M^3_1x\,+\,M^2_1M_2x+ M_1M_2M_1x \,+\, M_1M^2_2x$ and $M_2M^2_1x\,+\,M_2M_1M_2x \,+\, M^2_2M_1x \,+\, M^3_2x$ etc. Here again, the linear algebraic interpretation would be to look at the eigenspaces of the matrix polynomials. This line of argument holds if we increase the fixed filters to $w$, i.e., $M_1,\ldots, M_w$. An immediate argument against the case of fixed filters are that (Q-i) filters are not fixed they are learnt and (Q-ii) they are not the same across layers. We now argue that (Q-i) and (Q-ii) might not be as difficult as they seem. While the space of filters is uncountably infinite, if one looks at their `image processing' functions such as edge detection, sharpening, blurring etc (say there are some $F_1,\ldots, F_K$ such finite functionalities which are of interest), then it is possible to bin the filters in a layer into say any one of $F_1,\ldots, F_K$ `image processing' functionalities. Then we can interpret the meaning of the feature extracted out of operations such as $F_{i_1} \odot F_{i_2}\odot \ldots \odot F_{i_l}x$ (where $l$ stands for layer, and $F_{i_1}\ldots,F_{i_l}\in\{F_1,\ldots, F_K\}$). 


