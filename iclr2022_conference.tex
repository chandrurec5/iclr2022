\documentclass{article} % For LaTeX2e
\usepackage{iclr2022_conference,times}
\usepackage{hyperref}
\usepackage{url}
\input{pack}
% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}





\title{Duality simplifies deep neural networks with rectified linear units}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Antiquus S.~Hippocampus, Natalia Cerebro \& Amelie P. Amygdale \thanks{ Use footnote for providing further information
about author (webpage, alternative address)---\emph{not} for acknowledging
funding agencies.  Funding acknowledgements go at the end of the paper.} \\
Department of Computer Science\\
Cranberry-Lemon University\\
Pittsburgh, PA 15213, USA \\
\texttt{\{hippo,brain,jen\}@cs.cranberry-lemon.edu} \\
\And
Ji Q. Ren \& Yevgeny LeNet \\
Department of Computational Neuroscience \\
University of the Witwatersrand \\
Joburg, South Africa \\
\texttt{\{robot,net\}@wits.ac.za} \\
\AND
Coauthor \\
Affiliation \\
Address \\
\texttt{email}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}



\maketitle

\input{abstract}
%Despite their success deep neural networks (DNNs) are still largely considered as black boxes. In this paper, we present a simplified and improved understanding of DNNs with rectified linear units (ReLUs). Unlike several methods which ‘explain’ the inner workings of DNNs by building simpler local models, our approach is to ‘dismantle’ the DNN with ReLU into separate components. In particular, we focus on the gating property (i.e., on/off states) of the ReLUs. We build on the prior work by Lakshminarayanan & Singh (2020), who developed a novel dual view, wherein the gates are dismantled from the weights. In this paper, we carry forward this dismantling, where the gates in a DNN are dismantled layer-by-layer, and finally the gates in a layer are dismantled unit-by-unit. The key simplification is in our main claim that gates are indeed the most fundamental entities in DNNs with ReLUs. We provide theoretical basis for this claim and justify the same in experiments. Based on our theory and experiments, we argue that a DNN with ReLU into three functionalities namely (i) gating (ii) pre-activation generation and (iii) weights. While a standard DNN with ReLU is such that these three functionalities are shared/entangled between its weights and activation, we propose a novel modification wherein we disentangle the three components.



\section{Introduction}

Despite their success deep neural networks (DNNs) are still largely considered as black boxes. In this paper, we present a simplified and improved understanding of DNNs with rectified linear units (ReLUs). Our approach is to identify and understand the \emph{functional components} in a DNN with ReLUs in a step by step manner.  Based on our understanding of such functional components, we finally propose a novel way to put them together in an entirely interpretable way thereby removing the `black box'-ness. Our work builds on top of prior work by \cite{npk} who developed a \emph{dual view} for fully connected DNNs with ReLUs. Our approach is in contrast to approaches that \emph{explain} the \emph{decisions} of a DNN by building simpler local models.


\subsection{Dual View: First step in understanding the role of gates}

The primal view is that computation in a DNN proceeds from the input to output in a layer-by-layer manner. In the dual view, the computation in a DNN is broken down into paths, and the output is a summation of the individual path contributions. For this purpose, the dual view exploits the gating property of ReLU, that is, the \emph{on/off} or \emph{active/inactive} states of the ReLUs. A path starts at an input node, passes through a weight and a ReLU in each layer until it reaches the output node. A path is active all the gates in that path active, and its contribution is equal to the product of the signal at the input node and the weights in the path. In the dual view, it is natural to think that depending on the input, some subset of the paths get activated, and computation from input to output is restricted to the `active' subnetwork consisting of such active paths. Thus, the gates are input dependent and the weights remain the same across inputs. However, weights are triggering the gates in the first place. So, in order to separately the understand of their roles, the gates are also physically separated from the weights in \emph{deep gated network} (DGN) setup, wherein, the input dependent gates are generated in a so called \emph{feature network} (which a DNN with ReLU whose sole  is  generation of the gates) and then gating signals are applied as external masks to a so called \emph{value network} which carries out the input to output computation. Having separated the gates and the weights, the role of gates and active subnetworks was investigated in theory and experiments.

\textbf{Neural Path Kernel (Theory).} A \emph{neural path kernel} (NPK) is defined and is equal to the \emph{Hadamard} product of the input Gram matrix and a correlation matrix which measures the overlap between the sub-networks active for the various input pairs. Prior results  \cite{arora2019exact,cao2019generalization,ntk} have shown the equivalence of an infinite width DNN trained using gradient descent and its corresponding \emph{neural tangent kernel} (NTK) matrix, the Gram matrix of the gradient of the network output with respect to the weights. It was shown that when the gates and weights are separated, with the gates being fixed and only the weights trained, under randomised initialisation, in the limit of infinite width, the NTK becomes equal to (but for a scalar term) the NPK. This equivalence between NTK and NPK analytically characterises the role of the active subnetworks.


\textbf{Gate Learning (Experiments).}  It was shown active subnetworks are learnt during training and it improves test accuracy.  Using the gates from a pre-trained DNN as external masks one can retrain the weights and match the test accuracy of the original pre-trained DNN with ReLU. 
%This implies that most useful input dependent information is in the gates. 
It was shown that if gates from a randomly initialised (instead of pre-trained) DNN are used as external masks, and the weights are trained, the test accuracy drops significantly. 
%This implies that gates are learnt when a DNN with ReLU is trained and such learning improves generalisation.

\subsection{Contribution and Organisation}
 
We pursue two goals (i) pedagogical: here the pursuit is not propose new methods to beat the state of the art, but to improve our understanding of basic functional components namely weights, activation, depth, width, convolutions with global pooling  and skip connection (ii) practical: here the pursuit is to build a white box model without significance performance loss with respect to state of the art. The pedagogical goal is to drive home the message that, even though the primal and dual views are mathematically equivalent, when compared to the primal, the dual view is a natural and simple way to interpret and understand the inner workings of DNNs with ReLUs. The pedagogical goal is achieved in the following two steps. 

Step 1 (\Cref{sec:fc}): We first theoretically investigate the structural properties of NPK by refining and extending the prior work on dual view. \Cref{th:fc} shows that \emph{correlation of gates is key}, and in particular that each layer is associated with a base kernel which measures the average correlation of gates, and the NPK is a \emph{Hadamard} product of the input Gram matrix and the base kernels. This implies that the role of ReLU is gating, the primal role of weights is to trigger the ReLUs, the role of width is averaging and the role of depth is to form a product kernel. In \Cref{th:conv} we show that the role of convolutions with global pooling is to provide rotational invariance to the NPK. In \Cref{th:res} we show that the role of skip connections is to provide an ensemble structure to NPK.

Step 2  (\Cref{sec:exp}): Having refined and extended the NPK in theory, we empirically investigate two interesting scenarios, first one is a counter intuitive setting and second one is an open question from \cite{randlabel}. We describe these below.

\quad$\bullet$ We destroy the layer by layer structure by permuting the gates or even tiling the gates of all layers and rotating them arbitrarily. We empirically observe that this does not cause performance to degrade at all. Since, it could be argued that the network still manages to recover in a layer by layer manner despite such layer permutations. In order to eliminate this argument, we shut off the input as well by making the input to the value network to be 1. While this is perhaps surprising and counter-intuitive to the primal view that progressively complicated structures are being in a layer by layer manner, however, from \Cref{th:fc} it is known that  correlation of gates is key and operations destroy the layer by layer structure do not destroy the correlation of the gates, and hence the empirical observations are reconciled readily in the dual view.

\quad$\bullet$ We show that upstream training with random labels followed by downstream training with true labels degrades test accuracy because the random labels affects the gates. This degradation of test accuracy was an open question in \cite{randlabel}.


In all results above, the gates were generated by a feature network which was a DNN with ReLUs.  Based on our understanding, we propose to replace the ReLUs with identity activations giving rise to a white box architecture called \texttt{DGN-NO-ACT}. Once the ReLU non-linearity is removed, the other operations such as convolution (which is  linear), pooling and batch norm (bias and scaling) are well understood and interpretable in `image processing' terms. 





%In this paper, we carry forward this dismantling, where the gates in a DNN are dismantled layer-by-layer, and finally the gates in a layer are dismantled unit-by-unit. The key simplification is in our main claim that gates are indeed the most fundamental entities in DNNs with ReLUs. We provide theoretical basis for this claim and justify the same in experiments. Based on our theory and experiments, we argue that a DNN with ReLU into three functionalities namely (i) gating (ii) pre-activation generation and (iii) weights. While a standard DNN with ReLU is such that these three functionalities are shared/entangled between its weights and activation,  we propose a novel modification wherein we disentangle the three components. 

% separate the functionalities to dedicated for (i) feature generation (without any hidden units), (ii) gating, and (iii) weights. 

%Due to this separation of functionalities network is entirely interpretable by design. Now, we first summarise the main results in the prior work by \cite{npk} who developed a dual view to understand the role of gates in DNNs with ReLUs, followed by the specific contributions in this paper.





%Our main claim is that gates are indeed the most fundamental entities in such DNNs. We provide theoretical basis for the claim and justify the same in experiments. Based on this claim, we propose a novel modification wherein the deep network has separate components to dedicated specifically to address (i) feature generation (without any hidden units), (ii) gating, and (iii) the weights in a decoupled manner. Due to this decoupled network is entirely interpretable by design. Now, we first summarise the main results in the prior work by \cite{npk} who developed a dual view to understand the role of gates in DNNs with ReLUs, followed by the specific contributions in this paper.





\bibliography{refs}
\bibliographystyle{iclr2022_conference}

\appendix
\section{Appendix}
You may include other additional sections here.

\end{document}

