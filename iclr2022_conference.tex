\documentclass{article} % For LaTeX2e
\usepackage{iclr2022_conference,times}
\usepackage{hyperref}
\usepackage{url}
\input{pack}
% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}





\title{Duality simplifies deep neural networks with rectified linear units}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Antiquus S.~Hippocampus, Natalia Cerebro \& Amelie P. Amygdale \thanks{ Use footnote for providing further information
about author (webpage, alternative address)---\emph{not} for acknowledging
funding agencies.  Funding acknowledgements go at the end of the paper.} \\
Department of Computer Science\\
Cranberry-Lemon University\\
Pittsburgh, PA 15213, USA \\
\texttt{\{hippo,brain,jen\}@cs.cranberry-lemon.edu} \\
\And
Ji Q. Ren \& Yevgeny LeNet \\
Department of Computational Neuroscience \\
University of the Witwatersrand \\
Joburg, South Africa \\
\texttt{\{robot,net\}@wits.ac.za} \\
\AND
Coauthor \\
Affiliation \\
Address \\
\texttt{email}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}



\maketitle

\input{abstract}
%Despite their success deep neural networks (DNNs) are still largely considered as black boxes. In this paper, we present a simplified and improved understanding of DNNs with rectified linear units (ReLUs). Unlike several methods which ‘explain’ the inner workings of DNNs by building simpler local models, our approach is to ‘dismantle’ the DNN with ReLU into separate components. In particular, we focus on the gating property (i.e., on/off states) of the ReLUs. We build on the prior work by Lakshminarayanan & Singh (2020), who developed a novel dual view, wherein the gates are dismantled from the weights. In this paper, we carry forward this dismantling, where the gates in a DNN are dismantled layer-by-layer, and finally the gates in a layer are dismantled unit-by-unit. The key simplification is in our main claim that gates are indeed the most fundamental entities in DNNs with ReLUs. We provide theoretical basis for this claim and justify the same in experiments. Based on our theory and experiments, we argue that a DNN with ReLU into three functionalities namely (i) gating (ii) pre-activation generation and (iii) weights. While a standard DNN with ReLU is such that these three functionalities are shared/entangled between its weights and activation, we propose a novel modification wherein we disentangle the three components.



\section{Introduction}

Despite their success deep neural networks (DNNs) are still largely considered as black boxes. In this paper, we present a simplified and improved understanding of DNNs with rectified linear units (ReLUs). Our approach is to \emph{dismantle} a DNN with ReLUs in a step by step manner to identify its \emph{functional components} and understand their role. This is in contrast to approaches that \emph{explain} the \emph{decisions} of a DNN by building simpler local models. Based on our understanding of such functional components, we finally propose a novel way to put them together in an entirely interpretable way thereby removing the `black box'-ness. Our work builds on top of prior work by \cite{npk} who developed a \emph{dual view} for fully connected DNNs with ReLUs.


\subsection{Dual View: First step in understanding the role of gates}

The primal view is that computation in a DNN proceeds from the input to output in a layer-by-layer manner. In the dual view, the computation in a DNN is broken down into paths, and the output is a summation of the individual path contributions. For this purpose, the dual view exploits the gating property of ReLU, that is, the \emph{on/off} or \emph{active/inactive} states of the ReLUs. A path starts at an input node, passes through a weight and a ReLU in each layer until it reaches the output node. Each path is active all the gates in that path active, and its contribution is equal to the product of the signal at the input node and the weights in the path. In the dual view, it is natural to think that depending on the input, some subset of the paths get activated, and computation from input to output is restricted to the `active' subnetwork consisting of such active paths. Thus, the gates are input dependent and the weights remain the same across inputs. However, weights are triggering the gates in the first place. So, in order to separately the understand of their roles, the gates are also physically separated from the weights in \emph{deep gated network} (DGN) setup, wherein, the input dependent gates are generated in a so called \emph{feature network} (which a DNN with ReLU whose sole  is  generation of the gates) and then gating signals are applied as external masks to a so called \emph{value network} which carries out the input to output computation. Having separated the gates and the weights, the role of active subnetworks was investigated in theory and experiments.

\textbf{Kernel For Subnetworks (Theory).} A \emph{neural path kernel} (NPK) is defined and is equal to the \emph{Hadamard} product of the input Gram matrix and a correlation matrix which measures the overlap between the sub-networks active for the various input pairs. Prior results  \cite{arora2019exact,cao2019generalization,ntk} have shown the equivalence of an infinite width DNN trained using gradient descent and its corresponding \emph{neural tangent kernel} (NTK) matrix, the Gram matrix of the gradient of the network output with respect to the weights. It was shown that when the gates and weights are separated, with the gates being fixed and only the weights trained, under randomised initialisation, in the limit of infinite width, the NTK becomes equal to (but for a scalar term) the NPK. This equivalence between NTK and NPK analytically characterises the role of the active subnetworks.


\textbf{Subnetwork Learning (Experiments).}  It was shown active subnetworks are learnt during training and it improves test accuracy.  Using the gates from a pre-trained DNN as external masks one can retrain the weights and match the test accuracy of the original pre-trained DNN with ReLU. 
%This implies that most useful input dependent information is in the gates. 
It was shown that if gates from a randomly initialised (instead of pre-trained) DNN are used as external masks, and the weights are trained, the test accuracy drops significantly. 
%This implies that gates are learnt when a DNN with ReLU is trained and such learning improves generalisation.

\subsection{Contribution and Organisation}
 
Prior work by \cite{npk} dismantled the DNN into gates and weights. However, in their theoretical result the role of the gates was implicit/indirect and the active subnetworks play an explicit/direct role. Also, in their experiments the active subnetwork structure needs to be preserved. In this paper, we dismantle the subnetwork structure, and obtain theoretical as well as empirical characterisation solely in terms of the gates themselves. 


$\bullet$  \textbf{Kernel For Gates} \Cref{sec:fc} contains all our theoretical results and the focus is on the gates. In \Cref{th:main}, we simplify their NPK expression to make the role of gates explicit. In particular, we show that each layer is associated with a base kernel which measures the average correlation of gates, and the NPK is a \emph{Hadamard} product of the input Gram matrix and the base kernels. We note that the role of width is averaging and the role of depth is to form a product kernel. In \Cref{th:conv,res} we extend the dual view to show that in the presence of convolutions with global pooling the NPK is rotationally invariant, and in the presence of skip connections the NPK has an ensemble structure. The additional structures in \Cref{th:conv,res} provide a theoretical explanation as to why networks with why  convolutions with pooling and skip connections might be better than vanilla fully connected counterparts.

$\bullet$ \textbf{Destroying Structure}. In the primal view, the widely held understanding that in a DNN progressively sophisticated information in learnt in a layer-by-layer manner. In \Cref{sec:exp} we verify the expression in \Cref{th:fc} by showing the two surprising and counter intuitive empirical result that go against the primal view. When gates are applied as external masks, and we show that layer-by-layer structure can be destroyed without loss of test accuracy.  Secondly, we show that the output computation does not need the input at all. We argue that these empirical results are intuitive and natural in the dual view.

$\bullet$ \textbf{Learning With Random Labels}. Prior work examined the role of learning in the gates/active subnetworks in improving test accuracy. Here, we examine the learning in the gates with random labels, and  show that  training with random labels adversely affects the learning in the gates causing a degradation in test accuracy. This settles the open question in \Cref{randlabel} related to degradation in test accuracy due to upstream training with random labels.

$\bullet$ \textbf{Entirely Intepretable Deep Network}. In DGN setup in our results above as well as prior results by \cite{npk}, the gates are generated by a feature network which is a DNN with ReLU. In \Cref{sec:}, we further dismantle the pre-activation from gating, i.e., the feature network has only identity activations, and we provide $\mathbf{1}$ as input to the value network. Thus from input till the gates involves only well known and entirely interpretable transformations such as convolutions, pooling and batch norm all of which are readily interpreted in `image processing' terms. The overall interpretation is that the trigger to the gates are based on well standard `image processing' operations, the gates then select the active subnetworks/paths for each input and the value network just learns to produce the output using these paths all which starting from $\mathbf{1}$ at their input.




%In this paper, we carry forward this dismantling, where the gates in a DNN are dismantled layer-by-layer, and finally the gates in a layer are dismantled unit-by-unit. The key simplification is in our main claim that gates are indeed the most fundamental entities in DNNs with ReLUs. We provide theoretical basis for this claim and justify the same in experiments. Based on our theory and experiments, we argue that a DNN with ReLU into three functionalities namely (i) gating (ii) pre-activation generation and (iii) weights. While a standard DNN with ReLU is such that these three functionalities are shared/entangled between its weights and activation,  we propose a novel modification wherein we disentangle the three components. 

% separate the functionalities to dedicated for (i) feature generation (without any hidden units), (ii) gating, and (iii) weights. 

%Due to this separation of functionalities network is entirely interpretable by design. Now, we first summarise the main results in the prior work by \cite{npk} who developed a dual view to understand the role of gates in DNNs with ReLUs, followed by the specific contributions in this paper.





%Our main claim is that gates are indeed the most fundamental entities in such DNNs. We provide theoretical basis for the claim and justify the same in experiments. Based on this claim, we propose a novel modification wherein the deep network has separate components to dedicated specifically to address (i) feature generation (without any hidden units), (ii) gating, and (iii) the weights in a decoupled manner. Due to this decoupled network is entirely interpretable by design. Now, we first summarise the main results in the prior work by \cite{npk} who developed a dual view to understand the role of gates in DNNs with ReLUs, followed by the specific contributions in this paper.





\bibliography{refs}
\bibliographystyle{iclr2022_conference}

\appendix
\section{Appendix}
You may include other additional sections here.

\end{document}

