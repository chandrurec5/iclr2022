\section{Introduction}\label{sec:intro}
Despite their success deep neural networks (DNNs) are still largely considered as black boxes. The commonly held view (which we call the primal view) is that, \emph{starting from the input each layer performs a non-linear transformation via activations and sophisticated structures are learnt layer-by-layer}. The conceptual roadblock with the primal view in relation to the `black box'-ness is that, only the final layer is linear and amenable to a feature/weight interpretation, and yet the hidden features i.e., the penultimate layer outputs are not interpretable themselves. In this paper, we focus on DNNs with rectified linear units (ReLUs). For such networks, we side step the roadbloack using the dual view of \cite{npk}. In this paper, we combine the primal dual views to reduce `black box'-ness issue to that of understanding a two linear structures.

%In this paper, we focus on DNNs with rectified linear units (ReLUs). For such networks, we argue that the `black box'-ness issue is mostly conceptual one arising from the primal view which is partial and incomplete. The primal view is that, increasingly sophisticated features are learnt in a layer-by-layer manner. The conceptual issue here is that, only the final layer is linear and amenable to a feature/weight interpretation, and yet the hidden features i.e., the penultimate layer outputs are not interpretable themselves. 
%The conceptual issue with the primal view is that, only the final layer is linear and amenable to a feature/weight interpretation, and even then, the hidden features i.e., the penultimate layer outputs, obtained as a result of several non-linear operations on the input are not interpretable themselves. 
%Recently \cite{npk} used the \emph{dual view} to investigate the role of gates (i.e., \emph{on/off} state) of the ReLUs. In this paper, we combine the primal dual views to reduce `black box'-ness issue to that of understanding a two linear structure (one each for primal and dual) and a gating mechanism that is sandwidched between the two.

%The dual view proposes an alternative feature decomposition, by projecting the computation onto the dimnesion of the paths. A path starts from an input node, passes through a weight and a ReLU in each layer and ends at an output node. A path is `active' (i.e., `on') if all the gates in the path are `active' (i.e. `on). Each path, if active, transmits the signal at its input node, which on its way to the output is scaled by a `neural path value' (NPV) which is equal the product of the weights in the path. For a given input, a `neural path feature' (NPF) is defined to be the product of signal at the input node and the path activity. The NPF and NPV are vectors in $\R^{\text{total.no.paths}}$, and the ouput is the inner product of these NPF and NPV. Note that the gates dictate the path activity -- the NPFs are known if the input and the gates are known. So, there is no need for any extra procedure to compute the NPFs. 

The dual view breaks down the computations path-by-path, and expresses the output as the inner product of a `path feature' and a `path value' (both vectors in $\R^{\text{total\,paths}}$).
%The dual view proposes an alternative feature decomposition, by projecting the computation onto the dimnesion of the paths. 
A path starts from an input node, passes through a weight and a ReLU in each layer and ends at an output node. 
A path is `active'  if all the gates in the path are `active' (i.e. `on'). Each path, if active, transmits the signal at its input node, which on its way to the output is scaled by a `path value' which is equal the product of the weights in the path. The `path feature' is the product of signal at the input node and the path activity. %The `path feature' and `path value' are vectors in , and the ouput is the inner product of these two. Note that the gates dictate the path activity -- the `path features' are known if the input and the gates are known. So, there is no need for any extra procedure to compute the `path features'. 


%Before we can talk about the practical use of the NPFs, we need to ask "how good are NPFs as features?", i.e., suppose we have the NPFs in hand, is it possible to learn the NPV and if so how effective will be that model. A trivial answer it that in a DNN with ReLUs the NPF and NPV are learnt simultaenous, so NPFs are indeed good features. However, it is also possible to check in a standalone manner as to whether NPFs are good features. This is achieved by a deep gated network (DGN) setup \citep{npk}, where the gates (i.e., NPFs) are generated by in the feature network, and are used as external masks in a different network called the value network (whose weights specify the NPV) which is responsible for the input to the output computation. It was shown that using the gates (i.e., NPFs) from a pre-trained DNN (by letting it to be the feature network), and training the NPV separately in the value network, one can match the performance of the pre-trained DNN. In short, NPFs are indeed useful features.


%Before we can talk about the practical use of the `path features', we need to ask "how good are these as features?", i.e., suppose we have them in hand, is it possible to learn the `path value' and if so how effective will be that model. A trivial answer it that in a DNN with ReLUs the `path feature' and `path value' are learnt in the same network, so `path features' are indeed good. However, it is also possible to check in a standalone manner the goodness of `path features'. This is achieved by a deep gated network (\texttt{DGN}) setup \citep{npk}, where the gates (i.e., `path features') are generated by in the feature network, and are used as external masks in a different network called the value network (whose weights specify the `path value') which is responsible for the input to the output computation. It was shown that using the gates (i.e., `path features') from a pre-trained DNN (by letting it to be the feature network), and training the `path value' separately in the value network, one can match the performance of the pre-trained DNN. In short, `path features' are indeed good.

%If the path-by-path conceptualisation of the dual view were to be non-vacuous, then any alternative setup that achives this should be good enough. 
An alternative way to compute path-by-path  is via the deep gated network (DGN) setup \citep{npk}. A DGN has two networks, a feature network which is a DNN with ReLUs, and a value network with gated linear units (GaLUs) \citep{sss}, which need external gating signals. The gates are generated in feature network, and are used as external gating signals to switch `on/off' the GaLUs (and hence the paths) in the value network  which computes the output from input. In other words, the input and the gates of the feature network realises the `path feature', and the value network realises the `path value' and the inner product of the `path feature' and `path value'. DGN being an alternative to DNN, the following were answered by \cite{npk}.

As an alternative model, a DGN performs \emph{only marginally} worse than its corresponding DNN. However, the primary use of a DGN was to characterise the information in the gates of a DNN. It was showed that \emph{gates contain the most useful information}: training the value network of a DGN with a pre-trained DNN as its feature network, matches the performance of the pre-trained DNN. It was also shown that \emph{gates are learnt during training} and that it improves test accuracy.

%\emph{Is DGN better than DNN?}
%%
%It was shown a DGN train from scratch (i.e., randomised initialisation of both feature and value network) performs \emph{only marginally} worse than DNN. DGN approximated DNN empirically.

%\emph{What is the use of DGN?}

%The primary use of a DGN was to characterise the information in the gates of a DNN. They showed that \emph{gates contain the most useful information}: training the value network of a DGN with pre-trained DNN as its feature network, matches the performance of the pre-trained DNN. It was also shown that \emph{gates are learnt during training} and that it improves test accuracy.

%The secondary use is that a DGN serves as an approximation (to a DNN) which is amenable to theoretical analysis.  %In other words, if we have the `path feature' (via the gates of feature network) then we can train `path value' (in the value network). This implies, in practice, `path feature' is akin to feature and `path value' is akin to weights in a standard linear model, wherein, if the feature is known the weigths can be trained. 


%Using the DGN, \cite{npk} confirmed the non-vacuousness of the dual view by asking (i) \emph{Q1: suppose we have the `path features' in hand, is it possible to learn the %path value' separately?} and (ii) \emph{Q2: can the DGN learn the `path features' and `path value' as good as the DNN}.

%1. \emph{Answer to Q1}.  

%2. \emph{Answer to Q2}. 

% Based on \emph{Answer to Q1}, they concluded that most useful information is in the gates, i.e., the `path features'. They also showed that gates are learnt during training which improves generalisation.
%Before we can talk about the practical use of the `path features', we need to ask "how good are these as features?", i.e., suppose we have them in hand, is it possible to learn the `path value' and if so how effective will be that model. A trivial answer it that in a DNN with ReLUs the `path feature' and `path value' are learnt in the same network, so `path features' are indeed good. However, it is also possible to check in a standalone manner the goodness of `path features'. 
 


 
\begin{comment}
In a DNN with ReLUs, both the `path feature' and `path value' are dictated by the same set of weights. In order to separately understand the roles of the th feature' and `path value', \cite{npk} used a deep gated network setup, wherein, one network computes the gates (i.e., `path features'), the gates are then applied as external masks to turn `on/off' the paths in a different network whose weights specify the `path value' and is resposible for the input to output computation. Using the DGN, \cite{npk} showed the following  results:

1. Gates (i.e., `path features') hold the most useful information. Using the gates of a pre-trained DNN as external masks and training the weights recovers the performance of the pre-trained DNN.

2. Gates (i.e., `path features') are learnt during training of a DNN with ReLUs: gates from a pre-trained DNN perform better than the gates from an randomly initialised DNN.

3. Gates (`path features') and weights ('path values) can be learnt separately.

4. The gates are formally characterised by the so called \emph{neural path kernel} (NPK) which is equal to the \emph{Hadamard} product of the input Gram matrix and a correlation matrix which measures the overlap between the sub-networks of paths active for the various input pairs.
\end{comment}


%We now discuss the deep gated network (DGN) setup which combines the primal and the dual views. In a DNN with ReLUs, both the `path feature' and `path value' are dictated by the same set of weights. On the contrary, in a DGN, there are two networks, a \emph{feature network} which generates the gates, which are then applied as external masks to turn `on/off' the paths in a different network called \emph{value network} whose weights specify the `path value' and is resposible for the input to output computation. In a DGN, the feature network corresponds to the primal view (since the gates are generated in a layer by layer manner) and the value network corresponds to the dual view. Note that by specifing the gates automatically specifies the `path features' as well. DGN setup can be used in two mode. First mode is solely for ablation studies, to separately measure the information in the gates of a pre-trained DNN: fix the feature network to be the pre-trained ReLU and an train only the value network. Secondly mode is an alternative to DNNs with ReLUs: both the feature and value network are trained simultaenous. Using the DGN, \cite{npk} showed the following interesting results:

%The dual view decomposes the computation in a DNN into two parts (i) computation in the gates which is input dependent and (ii) computation in the weights which is the same across inputs. Based on this decomposition, the gates are separated from the weights. The information in the gates is then characterised by keeping the gates fixed, applying them as external masks and then training the weights. In this setting, the following interesting results were shown:

%1. \textbf{ Most information is in the gates}. Using the gates from a pre-trained DNN as external masks one can retrain the weights and match the test accuracy of the original pre-trained DNN with ReLU. 

%2. \textbf{Gate Are Learnt.} It was shown that gates are learnt during training and it improves test accuracy. It was shown that if gates from a randomly initialised (instead of pre-trained) DNN are used as external masks, and the weights are trained, the test accuracy drops significantly. This implies that gates are learnt when a DNN with ReLU is trained.
  
%This implies that most useful input dependent information is in the gates. 

%3. \textbf{Neural Path Kernel For Subnetwork.} For any given input, computation from input to output happens via the `active subnetwork' consisting of active paths.
%It was show that the information stored in the gates is analytically characterised by \emph{neural path kernel} (NPK) which is equal to the \emph{Hadamard} product of the input Gram matrix and a correlation matrix which measures the overlap between the sub-networks active for the input pairs.

%\textbf{Our Contributions.} In the first part of the paper, we look at the issue of `black box'-ness. As mentioned earlier, this issue is conceptual. The main misconception is that \emph{starting from the input each layer performs a non-linear transformation via activations and sophisticated structures are learnt layer-by-layer}. The two things, i.e., `layer-by-layer' and `non-linear activations' are separately correct; the misconception is the need for the two to be entangled. DGN \citep{npk} takes the first step in the disentanglement, by separating the primal and the dual: the feature network computes the gates layer-by-layer (primal) and the value network computes the output path-by-path (dual). We identify two issues with the DGN and rectify it to obtain the novel \texttt{DGN-NO-ACT} which is a white box model. We discuss the issue and our proposals to address them.

%\emph{\textbf{Issue 1:} The feature network which is a DNN with ReLUs is a `black box'.}

%\emph{\textbf{Our Solution:}} We replace the ReLUs with identity activations, and use the pre-activations to trigger the gates, which are then provided to the value network. The transformation from the input to pre-activation has no non-linear activations and interpretable via linear algebraic tools.

%\emph{\textbf{Issue 2}: The value network might be learning sophisticated structures from the input in a layer-by-layer manner, and hence the value network is also a `black box'.}

%\emph{\textbf{Our Solution:}} We give a constant $\mathbf{1}$ as input to the value network, thereby removing any misconception that perhaps sophisticated structures are learnt layer-by-layer in the value network.


%\emph{\textbf{Issue 3:} Are we not impeding the layer-by-layer learning ability by providing a $\mathbf{1}$ input?}

%\emph{\textbf{Our Answer:}} No. All the layer-by-layer learning from input happens in the feature network (without use of non-linear activations), so we are not doing way with this aspect.


%\textbf{Our Contributions.} In the first part of the paper, we look at the issue of `black box'-ness. As mentioned earlier, this issue is conceptual. The main misconception is that \emph{starting from the input each layer performs a non-linear transformation via activations and sophisticated structures are learnt layer-by-layer}. The two things, i.e., `layer-by-layer' and `non-linear activations' are separately correct; the misconception is in the need for these two to be entangled. DGN \citep{npk} takes the first step in the disentanglement, by separating the primal and the dual: the feature network computes the gates layer-by-layer (primal) and the value network computes the output path-by-path (dual). We propose a novel white box model \texttt{DGN-NO-ACT} by modifying the DGN. The hightlights are:


\textbf{Our Contributions.} In the first part of the paper, we look at the issue of `black box'-ness. For this, we modify the DGN to propose a novel \texttt{DGN-NO-ACT} which decomposes into simpler structures. 

$\bullet$ \texttt{DGN-NO-ACT} = Primal Linearity + Gates + Dual Linearity

In a DGN, the primal layer-by-layer comptutation happens in the feature network. However, the issue is that the feature network is a DNN in which the linear matrix multiplication and non-linear activations are entangled. In our novel \texttt{DGN-NO-ACT}, we disentangle by replacing the ReLU activations in the feature network by identity activations thereby making the feature network `primal linear'. The gates are then activated by the pre-activations.  The value network \texttt{DGN-NO-ACT} is same as the one in DGN. The value network is linear in the `path' variables and yet, it is non-linear in the network weight. However, a kernel interpretation is obtained in the \emph{infinite width regime}. In essence, the primal linear structure  is interepretable via standard linear algebraic tools, and the dual has a kernel interpretation, and the gates serve the purpose of \emph{lifting} from  primal to dual.

%The main conceptual roadblock occurs because each layer entangles linear matrix multiplication and non-linear activations. In our novel \texttt{DGN-NO-ACT} (\Cref{sec:dgn-no-act}), we modify the DGN to achieve complete disentanglement. Specifically, we modify the feature network by replacing its ReLU activations by identity activations; this makes it primal linear. In a DGN, the value network is already dual linear, i.e, linear in the `path' variables. The key points are:

%$\bullet$ \textbf{Primal Linearity:} We replace the ReLUs in the feature network with identity activations. This makes the feature network free of activations and entirely linearly.  

%$\bullet$ \textbf{Gating:} The preactivations from the feature network are used to trigger the gates. The gates here are seen to play the role of handing off the computations from the primal to the dual.

%$\bullet$ \textbf{Dual Linearity:} We set the input to the value network, so that the `path value' has no input dependence. The value network can be described in simpled terms as:

%{\centering \emph{\textbf{The output is the summation of `path value' weighted by the `path activations'.}}\par}


%$\bullet$ In a \texttt{DGN-NO-ACT}, we defer a formal analysis of how the feature network learns useful gating patterns to future work. Instead, in \Cref{sec:primal}, we informally argue via standard linear algebra that the feature network might be looking at all possible spectral components of the input.

$\bullet$ Understading Primal Linearity in a \texttt{DGN-NO-ACT}

The transformation from the input to the pre-activation is linear, which in turn activate the gates. We defer a formal analysis of how the feature network learns useful pre-activations to future work. However, the linearity of feature network is itself a huge gain, because, we do not have to resort to `locally linear explainations' using other models.

$\bullet$ Understading Dual Linearity in DGN and \texttt{DGN-NO-ACT}

Prior work by \cite{ntk,arora2019exact,cao2019generalization} showed that training an infinite width DNN is equivalent to a kernel method with the so called \emph{neural tangent kernel} (NTK). \citep{npk} showed that in an infinite width fully connected DGN with its gates fixed, the NTK is equal (up to a scalar) to the so called \emph{neural path kernel}(NPK) (equal to the Gram matrix of the `path features'). An important property of the NPK which was unnoticed is that the NPK is invaraint to permutation of layers. In \Cref{th:fc}, we explicitise this invariance by rewriting the NPK as a \emph{Hadamard} product of the input Gram matrix and the base kernels (one per layer) which measures the average correlation of gates (in those layers). The expression also reveals that the role of depth is to provide the product structure and the role of width is averaging. %Thus, thanks to the primal dual separation, \Cref{th:fc} captures the roles of gates, depth and width in a single NPK expression. 
In \Cref{th:conv,th:res}, we extend the dual view to show that the NPK is rotationally invariant in the presence of convolutions with global pooling, and NPK is an ensemble in the presence of skip connections respectively. 

In \Cref{sec:exp}, we show that the value network does not learn in a layer-by-layer manner; this is important to convince that the value network is indeed linear in the dual `path' variables and computes path-by-path.
%here are two sources via which the input affects the value network, and we destroy both of them. Firstly, we provide the value network with a constant $\mathbf{1}$, ensuring that the only way input dependent information affects the value networks is via the other source namely the gates. 
We destroy the layer-by-layer structure by permuting the gates and also provide a constant $\mathbf{1}$ as input. We show that these does not cause a degradation in performance: counter intuitive and surprising from the primal view. However, results in \Cref{sec:theory} easily explains these.%the dual view easily explains these. %From the expression of \Cref{th:fc}, we know that the input Gram matrix will be a constant. However, the product of the base kernels still contain information about the input. Also, the permutation invariance of the product structure of the NPK allows us to destroy the layer-by-layer structure of the gates.

$\bullet$ In \Cref{sec:dgn-no-act-cifar}, we show that \texttt{DGN-NO-ACT}s corresponding to VGG and a ResNet model achieve greater than $90\%$ and close to $70\%$ accuracy on CIFAR-10 and CIFAR-100 respectively. 

%$\bullet$ In our novel \texttt{DGN-NO-ACT}, we achieve complete disentanglement of feature network, gating and value network. In the feature network, we replace the ReLUs with identity activations, and use the pre-activations to trigger the gates which are now external to the feature network. The gates are then provided to the value network. We give a constant $\mathbf{1}$ as input to the value network, and hence the `path value' is a vector inin $\R^\text{{total\,paths}}$ and contains no input dependent information. The `path feature' is a vector in $[0,1]^\text{{total\,paths}}$ which specifies the activation levels of each path. In simple terms,

%More importantly, the path activations are dictated by the gates, and transformation from the input to  pre-activation of the gates has no non-linear activations and interpretable via linear algebraic tools.

%$\bullet$ We show that \texttt{DGN-NO-ACT}s corresponding to VGG and a ResNet model achieve greater than $90\%$ and close to $70\%$ accuracy on CIFAR-10 and CIFAR-100 respectively. 

%While prior results \cite{} have also noted the ensemble nature, we believe \Cref{th:res} is more fundamental due to the fact that it is simply based on the presence of combinatorially many architectures caused as a direct consequnce of the skip connections. 

%$\bullet$ In \Cref{sec:theory}, we also revisit prior result for the full connected case. 


%\textbf{Simplifed Interpretation.} As we said, the main misconception is entangling the `layer-by-layer' and the non-linear activations. What we achieve is disentanglement: in our novel \texttt{DGN-NO-ACT} the feature network learns layer-by-layer, and the value network learns path-by-path, and the non-linerity, i.e., gating uses pre-activation from feature network and switches the paths `on/off' in the value network. The proposed \texttt{DGN-NO-ACT} has the following simple interpretation: `path value' is a vector in $\R^\text{{total\,paths}}$ and the `path feature' is a vector in $[0,1]^\text{{total\,paths}}$ which specifies the activation levels of each coordinate in `path value to produce the final output.


%\textbf{Prior Work.} The path-by-path dual view gives subnetwork interpretation. For each input, only a subset of gates are active, and correspondingly only a subnetwork of the paths are active (the `path feature' of the inactive paths is $0$). The Gram matrix of the `path features' is the so called the \emph{neural path kernel} (NPK) and is equal to the \emph{Hadamard} product of input Gram matrix and a correlation matrix which measures the overlap between the active subnetworks active for the various input pairs. \cite{npk} also showed that training the infinite width DGN (with gates fixed) is equivalent to a kernel method with the NPK.

%\textbf{Contribution I.} An important property of the NPK which was unnoticed is that the NPK is invaraint to permutation of layers. In \Cref{th:fc}, we remedy this by rewriting the NPK as a \emph{Hadamard} product of the input Gram matrix and the base kernels (one per layer) which measures the average correlation of gates (in that layer). In \Cref{th:conv} we show that the role of convolutions with global pooling is to provide rotational invariance to the NPK. In \Cref{th:res} we show that the role of skip connections is to provide an ensemble structure to NPK. 

%\textbf{Contribution II.} Once we have the gates from feature network, we show that destroying the layer-by-layer structure by permuting the layers before applying to the value network does not degrade the performance. It could be argued that the value network still manages to recover in a layer-by-layer manner despite such layer permutations. In order to eliminate this argument, we provide a constant $\mathbf{1}$ input to the value network, and observe that even this does not degrade performance. From the primal viewpoint, these are surprising and counter intuitive results. However, in the dual view these are readily reconciled. We know from \Cref{th:fc}, that a constant $\mathbf{1}$ input to the value network sets the input Gram matrix to be a constant, and the NPK is then simply the product of base kernels, which is invariant to layer permutations. In short, the value network indeed computes path-by-path.

%i.e., each path starts with $1$ at its input nodes, only the path activity is input dependent (since it is directly controlled by the gates). Using a constant $\mathbf{1}$ input does not degrade the performance either. 


%\emph{\textbf{Issue 2:} The feature network which is a DNN with ReLUs is a `black box'.}

%\textbf{Contribution III.}  We call this modified DGN as \texttt{DGN-NO-ACT}, and 
%\textbf{White Box.} The \texttt{DGN-NO-ACT} is entirely intrepretable and white box. The feature network generates the pre-activations to the gates without use of non-linear activations, the gates then switch the paths `on/off' to realise the `path features' in the value network which the just computes the inner product between the `path feature' and `path value'.

%In this paper, we improve and simplilfy the DGN to build an entirely interpretable white box DNN. The DGN serves as a good starting point,  in that, it combines the primal view: feature network generating the gates (i.e, `path features') in a layer by layer manner, and the dual view: the value network specifying the `path values' and computing the output (i.e., inner product). We remove the `black box'ness by (i) interpreting the  `path features', (ii) interpreting the  `path value' and (iii) removing non-linear activations in the feature network. 



% This permutation invariance was unnoticed in prior work, and is a key property of the gating features that has surprising and counter intuitive implications (see Q7 and A7 below). In \Cref{th:fc}, we essentially rewrite the prior NPK expression in a way that explicitly reveals this permutation invariance. The main highlight is that each layer is associated with a base kernel which measures the average correlation of gates, and the NPK is a \emph{Hadamard} product of the input Gram matrix and the base kernels. Here, the role of width is averaging and the role of depth is endowing the product structure from which the permutation invaraince ensues. In \Cref{th:conv} we show that the role of convolutions with global pooling is to provide rotational invariance to the NPK. In \Cref{th:res} we show that the role of skip connections is to provide an ensemble structure to NPK. 


%\textbf{Interpreting `Path Value'.}  As per the dual view, all that is there to the value network is `path value', \emph{can we rule out the possibility that the value network is learning sophisticated structures in a layer by layer manner?}. Since `path value' is akin to a weight vector it is interpretable in a straightforward manner, which means we only have the feature network to interpret.

In the second part of the paper (see \Cref{sec:randlabel}), we look at the following open question in \citep{randlabel}: \emph{when trained upstream with random labels followed by downstream training with true labels, the test accuracy degrades, why?} We show that the degradation in the test performance is because the gates, i.e., the `path feautres' degrade.

%Using the dual view, we interpret the roles of (i) width, (ii) depth, (iii) convolutions with global pooling and (iv) skip connections. For this, we refine and extend prior work to derive new results on the structure of NPK. We then present two new empirical results that are easily reconciled in the dual view (as opposed to the primal). These results are not meant to be techniques/tricks to improve performance that `beats state of the art'. The significance is that they showcase the power of dual view in resolving novel and unseen scenarios. We now list these contributions section-wise.

%$\bullet$ In \Cref{sec:theory}, we refine prior result on NPK (which depended on the correlation subnetworks) to show that NPK depends on the \emph{correlation of gates}. The main highlight is that each layer is associated with a base kernel which measures the average correlation of gates, and the NPK is a \emph{Hadamard} product of the input Gram matrix and the base kernels. This implies that the role of width is averaging and the role of depth is to form a product kernel. In \Cref{th:conv} we show that the role of convolutions with global pooling is to provide rotational invariance to the NPK. In \Cref{th:res} we show that the role of skip connections is to provide an ensemble structure to NPK.


%$\bullet$ In \Cref{sec:permute}, we present surprising results that are counter intuitive with respect to the primal view that progressively sophisticated structures are being in a layer by layer manner. Firstly, we show that destroying the layer by layer structure by permuting the gates does not cause performance to degrade at all. Secondly, we show that once the gates are obtained from the input, the network can be provided with a constant $\mathbf{1}$ input without degrading performance. We argue that these counter intuitive results are easily reconciled in the dual view.

%\emph{Part II: Open question related to training with random labels.}

% In \Cref{sec:randlabel}, we show that upstream training with random labels followed by downstream training with true labels degrades test accuracy because the random labels affects the gates. This degradation of test accuracy was an open question in \cite{randlabel}.


%\emph{Part III: Entirely interpretable and white box model.}

%In \Cref{sec:whitebox}, we propose a novel model by modifying  DNNs with ReLUs: we generate the pre-activation to the gates without any non-linear activations. Interpreted in the dual view, this novel model is entirely white box. We show white box models obtained by modifying VGG and a ResNet (in the proposed way) achieve greater than $90\%$ and close to $70\%$ on CIFAR-10 and CIFAR-100 respectively. 


%we propose a novel architecutre obtained by modifying DNNs with ReLUs, which, is conceptually same as DNNs with ReLUs but is entirely interpretable and white box. We effect the modification on a stardard model namely VGG and a ResNet model, to derive the corresponding white box models. We show these white box models achieve greater than $90\%$ and close to $70\%$ on CIFAR-10 and CIFAR-100 respectively. Here, the aim is to demonstrate that the new white box models improve interpretablity without significant loss in performance with respect to `state of the art'.


%We follow up this claim by completely removing the `black box'-n ess: we propose a novel architecutre, which, is conceptually same as DNNs with ReLUs but is entirely interpretable and white box. 
\begin{comment}
\subsection{Dual View}
In the dual view, the computation in a DNN is broken down into paths, wherein, each path starts from the input node, passes through a weight and a ReLU in each layer and ends at the output node. This gives a subnetwork intepretation: for any given input, computation from input to output happens via the `active subnetwork' consisting of active paths. This active subnetwork is in turn formed by the active gates in each layer. Each gate is a just a  simple `perceptron' which is described by the hyperplane of its incoming weights. As the the input passes through the layers, gates are turned on/off based on the angle between the layer input and the hyperplanes associated with the various gates in that layer. 


 The separation is achieved by a \emph{deep gated network} (DGN) setup, wherein, the input dependent gates are generated in a so called \emph{feature network} (which a DNN with ReLU whose sole  is  generation of the gates) and then gating signals are applied as external masks to a so called \emph{value network} which carries out the input to output computation. Having separated the gates and the weights, the role of gates and active subnetworks was investigated in theory and experiments.

%The main result  It was shown that most useful information is in the gates.  

%The dual view (i) gives simpler feature/value decomposition, (ii) gives a subnetwork intepretation and (iii) allows to separate the gates from the weights. We briefly described these below.

%Each path has a signal at its input node, which is gets transmitted to the output if the path is on (i.e., all the gates in the paths are on). On its way to the output, (if path is active) the input signal of a path is scaled by a `value' equal the product of the weights in the path. The input and the on/off of the path is encoded in a so called  and the scaling by the weights is encoded by the \emph{neural path value} (NPV $\in \R^{\text{total-paths}}$). This conceptual separation of computation in the gates encoded in the NPF and the computation in the weights in the NPV has the following favourable points:

%$\bullet$ \textbf{Simplicity.} The input and the gates of a path are  encoded in a \emph{neural path feature} (NPF $\in \R^{\text{total-paths}}$). The weights are encoded in a \emph{neural path value} (NPV $\in \R^{\text{total-paths}}$). The NPF coordinate of a path is $0$ if the path is inactive and is equal to the signal at the input node if the path is active.  The NPV coordinate of a path is the product of the weights in the path. The NPV of a path the scales signal at it input node, and the final DNN output is the inner product of NPF and NPV.

%$\bullet$ \textbf{Interpretability.} For any given input, computation from input to output happens via the `active subnetwork' consisting of active paths. This active subnetwork is in turn formed by the active gates in each layer. Each gate is a `perceptron' which is described by the hyperplane of its incoming weights. As the the input passes through the layers, gates are turned on/off based on the angle between the layer input and the hyperplanes associated with the various gates in that layer. 

%$\bullet$ \textbf{Separation.} 
%In a DNN with ReLUs, the weights play a dual role: (i) they decide the on/off activity of the gates, i.e., the NPF and they also dictate the NPV. 
%To understand the role of gates and weights separately, the gates are also physically separated from the weights in \emph{deep gated network} (DGN) setup, wherein, the input dependent gates are generated in a so called \emph{feature network} (which a DNN with ReLU whose sole  is  generation of the gates) and then gating signals are applied as external masks to a so called \emph{value network} which carries out the input to output computation. Having separated the gates and the weights, the role of gates and active subnetworks was investigated in theory and experiments.

1. \textbf{Gate Learning.} Most information is stored in the gates. Using the gates from a pre-trained DNN as external masks one can retrain the weights of the value network and match the test accuracy of the original pre-trained DNN with ReLU. It was shown that gates are learnt during training and it improves test accuracy.  
%This implies that most useful input dependent information is in the gates. 
%It was shown that if gates from a randomly initialised (instead of pre-trained) DNN are used as external masks, and the weights are trained, the test accuracy drops significantly. 
%This implies that gates are learnt when a DNN with ReLU is trained and

2. \textbf{Neural Path Kernel.} The information stored in the gates is analytically characterised by \emph{neural path kernel} (NPK) which is equal to the \emph{Hadamard} 
\end{comment}
\begin{comment}

\subsection{Dual View}
In the dual view, the computation in a DNN is broken down into paths, wherein, each path starts from the input node, passes through a weight and a ReLU in each layer and ends at the output node. This gives a subnetwork intepretation

 and allows to separate information in the gates from the weights. The main result  It was shown that most useful information is in the gates.  

%The dual view (i) gives simpler feature/value decomposition, (ii) gives a subnetwork intepretation and (iii) allows to separate the gates from the weights. We briefly described these below.

%Each path has a signal at its input node, which is gets transmitted to the output if the path is on (i.e., all the gates in the paths are on). On its way to the output, (if path is active) the input signal of a path is scaled by a `value' equal the product of the weights in the path. The input and the on/off of the path is encoded in a so called  and the scaling by the weights is encoded by the \emph{neural path value} (NPV $\in \R^{\text{total-paths}}$). This conceptual separation of computation in the gates encoded in the NPF and the computation in the weights in the NPV has the following favourable points:

%$\bullet$ \textbf{Simplicity.} The input and the gates of a path are  encoded in a \emph{neural path feature} (NPF $\in \R^{\text{total-paths}}$). The weights are encoded in a \emph{neural path value} (NPV $\in \R^{\text{total-paths}}$). The NPF coordinate of a path is $0$ if the path is inactive and is equal to the signal at the input node if the path is active.  The NPV coordinate of a path is the product of the weights in the path. The NPV of a path the scales signal at it input node, and the final DNN output is the inner product of NPF and NPV.

$\bullet$ \textbf{Interpretability.} For any given input, computation from input to output happens via the `active subnetwork' consisting of active paths. This active subnetwork is in turn formed by the active gates in each layer. Each gate is a `perceptron' which is described by the hyperplane of its incoming weights. As the the input passes through the layers, gates are turned on/off based on the angle between the layer input and the hyperplanes associated with the various gates in that layer. 

$\bullet$ \textbf{Separation.} 
%In a DNN with ReLUs, the weights play a dual role: (i) they decide the on/off activity of the gates, i.e., the NPF and they also dictate the NPV. 
To understand the role of gates and weights separately, the gates are also physically separated from the weights in \emph{deep gated network} (DGN) setup, wherein, the input dependent gates are generated in a so called \emph{feature network} (which a DNN with ReLU whose sole  is  generation of the gates) and then gating signals are applied as external masks to a so called \emph{value network} which carries out the input to output computation. Having separated the gates and the weights, the role of gates and active subnetworks was investigated in theory and experiments.

\textbf{Key Results.}  

1. \textbf{Gate Learning.} Most information is stored in the gates. Using the gates from a pre-trained DNN as external masks one can retrain the weights of the value network and match the test accuracy of the original pre-trained DNN with ReLU. It was shown that gates are learnt during training and it improves test accuracy.  
%This implies that most useful input dependent information is in the gates. 
%It was shown that if gates from a randomly initialised (instead of pre-trained) DNN are used as external masks, and the weights are trained, the test accuracy drops significantly. 
%This implies that gates are learnt when a DNN with ReLU is trained and

2. \textbf{Neural Path Kernel.} The information stored in the gates is analytically characterised by \emph{neural path kernel} (NPK) which is equal to the \emph{Hadamard} product of the input Gram matrix and a correlation matrix which measures the overlap between the sub-networks active for the input pairs.
\end{comment}
\begin{comment}

\textbf{Neural Path Feature.} In the dual view, the computation in a DNN is broken down into paths, which leads to an alternative feature/value decomposition. Each path has a signal at its input node, which is gets transmitted to the output if the path is on (i.e., all the gates in the paths are on). On its way to the output, (if transmitted) the input signal of a path is scaled by a `value' equal the product of the weights in the path. The input and the on/off of the path is encoded in a so called \emph{neural path feature} (NPF $\in \R^{\text{total-paths}}$) and the scaling by the weights is encoded by the \emph{neural path value} (NPV $\in \R^{\text{total-paths}}$). The DNN output for an input $x\in\R^{\din}$ , and parameter $\Theta\in\R^{\dnet}$ is given by:
\begin{align}
\texttt{DNN-OUTPUT(x)=}\ip{\texttt{NPF}_{\Theta}\texttt{(x),NPV}_{\Theta}}
\end{align}

\textbf{Subnetwork Interpretation.} For any given input, the NPF coordinate is zero for all the inactive paths. This provides a subnetwork based interpretation, that is, computation from input to output happens via the `active subnetwork' consisting of active paths.  



As as result, the Gram matrix of the NPFs called the \emph{neural path kernel} (NPK) is equal to the \emph{Hadamard} product of the input Gram matrix and a correlation matrix which measures the overlap between the sub-networks active for the various input pairs. 



\textbf{Self-Explanation.}


For this purpose, the dual view exploits the gating property of ReLU, that is, the \emph{on/off} or \emph{active/inactive} states of the ReLUs. A path starts at an input node, passes through a weight and a ReLU in each layer until it reaches the output node. A path is active all the gates in that path active, and its contribution is equal to the product of the signal at the input node and the weights in the path. In the dual view, it is natural to think that depending on the input, some subset of the paths get activated, and computation from input to output is restricted to the `active' subnetwork consisting of such active paths. Thus, the gates are input dependent and the weights remain the same across inputs. Holwever, weights are triggering the gates in the first place. So, in order to separately the understand of their roles, the gates are also physically separated from the weights in \emph{deep gated network} (DGN) setup, wherein, the input dependent gates are generated in a so called \emph{feature network} (which a DNN with ReLU whose sole  is  generation of the gates) and then gating signals are applied as external masks to a so called \emph{value network} which carries out the input to output computation. Having separated the gates and the weights, the role of gates and active subnetworks was investigated in theory and experiments.

\textbf{Neural Path Kernel (Theory).} A \emph{neural path kernel} (NPK) is defined and is equal to the \emph{Hadamard} product of the input Gram matrix and a correlation matrix which measures the overlap between the sub-networks active for the various input pairs. Prior results  \cite{arora2019exact,cao2019generalization,ntk} have shown the equivalence of an infinite width DNN trained using gradient descent and its corresponding \emph{neural tangent kernel} (NTK) matrix, the Gram matrix of the gradient of the network output with respect to the weights. It was shown that when the gates and weights are separated, with the gates being fixed and only the weights trained, under randomised initialisation, in the limit of infinite width, the NTK becomes equal to (but for a scalar term) the NPK. This equivalence between NTK and NPK analytically characterises the role of the active subnetworks.


\textbf{Gate Learning (Experiments).}  It was shown active subnetworks are learnt during training and it improves test accuracy.  Using the gates from a pre-trained DNN as external masks one can retrain the weights and match the test accuracy of the original pre-trained DNN with ReLU. 
%This implies that most useful input dependent information is in the gates. 
It was shown that if gates from a randomly initialised (instead of pre-trained) DNN are used as external masks, and the weights are trained, the test accuracy drops significantly. 
%This implies that gates are learnt when a DNN with ReLU is trained and such learning improves generalisation.
\end{comment}



%In all results above, the gates were generated by a feature network which was a DNN with ReLUs.  Based on our understanding, we propose to replace the ReLUs with identity activations giving rise to a white box architecture called \texttt{DGN-NO-ACT}. Once the ReLU non-linearity is removed, the other operations such as convolution (which is  linear), pooling and batch norm (bias and scaling) are well understood and interpretable in `image processing' terms. 


%We pursue two goals (i) pedagogical: here the pursuit is not propose new methods to beat the state of the art, but to improve our understanding of basic functional components namely weights, activation, depth, width, convolutions with global pooling  and skip connection (ii) practical: here the pursuit is to build a white box model without significance performance loss with respect to state of the art. The pedagogical goal is to drive home the message that, even though the primal and dual views are mathematically equivalent, when compared to the primal, the dual view is a natural and simple way to interpret and understand the inner workings of DNNs with ReLUs. The pedagogical goal is achieved in the following two steps. 





%In this paper, we carry forward this dismantling, where the gates in a DNN are dismantled layer-by-layer, and finally the gates in a layer are dismantled unit-by-unit. The key simplification is in our main claim that gates are indeed the most fundamental entities in DNNs with ReLUs. We provide theoretical basis for this claim and justify the same in experiments. Based on our theory and experiments, we argue that a DNN with ReLU into three functionalities namely (i) gating (ii) pre-activation generation and (iii) weights. While a standard DNN with ReLU is such that these three functionalities are shared/entangled between its weights and activation,  we propose a novel modification wherein we disentangle the three components. 

% separate the functionalities to dedicated for (i) feature generation (without any hidden units), (ii) gating, and (iii) weights. 

%Due to this separation of functionalities network is entirely interpretable by design. Now, we first summarise the main results in the prior work by \cite{npk} who developed a dual view to understand the role of gates in DNNs with ReLUs, followed by the specific contributions in this paper.





%Our main claim is that gates are indeed the most fundamental entities in such DNNs. We provide theoretical basis for the claim and justify the same in experiments. Based on this claim, we propose a novel modification wherein the deep network has separate components to dedicated specifically to address (i) feature generation (without any hidden units), (ii) gating, and (iii) the weights in a decoupled manner. Due to this decoupled network is entirely interpretable by design. Now, we first summarise the main results in the prior work by \cite{npk} who developed a dual view to understand the role of gates in DNNs with ReLUs, followed by the specific contributions in this paper.




 %Recent past has seen two paradigms namely `explainability' and `interpretabilty' to addres the issue of `black box'-ness. In the  `explaniablity' paradigm, one accepts as a fact that complex machine learning tasks might require complex black box models, however, resorts to \emph{post-hoc} \emph{explaination} of the \emph{decisions} of a DNN by building simpler local models. In the `interpretability' paradigm, one builds entirely interpretable white box models in the first place, thereby eliminating the need for \emph{post-hoc} explanations. 







