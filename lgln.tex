\section{LGLN:  Disentangling the computation in DNNs with ReLUs}\label{sec:lgln}
In this section, we turn to the issues of entanglement and `black box'-ness. We start by noting that the performance in the \emph{learnable gates} setting of the DGN show us that the DGN is not only conceptually similar to the DNN, but also empirically approximate to the DNN. In this paper, we modify the DGN to obtain the LGLN. In particular, the LGLN (\Cref{fig:lgln}) is same as the DGN in which (i) the ReLUs in the feature network are replaced with identity activations, and (ii) the input to the value network is $\mathbf{1}$ (see bottom network of LGLN in \Cref{fig:lgln} and the input to the value network of the DGN in \Cref{fig:dgn}). We now discuss how the LGLN disentangles the computations into two linear structures to show that the commonly held view of sophisticated structures are learnt in a layer-by-layer manner is misconceived.  From now on, as it is with the DGN, we will refer to the top part of the LGLN as its feature network and the bottom part as its value network. 



%Continuing with the `learning in gates + learning in the weights given gates' interpretation, using LGLN (\Cref{fig:lgln}), we disentangle both `learning in the gates' as well as `learning in the weights given gates'. 


%We now elaborate on why these changes are needed in the LGLN and how disentangling is achieved.

$\bullet$ \textbf{Disentangling Feature Network.}  %The gates themselves encode binary information and are triggered by their pre-activations.
In a DGN, the transformation from the input to the pre-activation to the gates happens through the hidden layers, in which the linear and non-linear operations are entangled. The hidden layers are not interpretable, and we have to fall back to sophisticated structures being learnt in the hidden layers as an explanation for why it is possible to learn useful pre-activations (as hence useful gates). In LGLN, the non-linear ReLUs are removed, and the transformation in linear, i.e., while the learning of pre-activations happens in a layer-by-layer manner, the learnt structures are no longer sophisticated and are interpretable in terms of standard linear algebra. We will call this \textbf{primal linearity}. Primal linearity is ensured via construction itself and needs no further theoretical justification, and in what follows, we experimentally verify in \Cref{sec:exp} that linearly learnt pre-activations does not degrade performance significantly, when compared to state of the art.

$\bullet$ \textbf{Disentangling Value Network.} %Note that the value network of the LGLN and that of the DGN are the same, except that in the DGN the value network is given $x\in\R^{\din}$ as the input and in the LGLN it is a constant $\mathbf{1}\in\R^{\din}$ instead. 
In a DGN, given that the input $x\in\R^{\din}$  is presented to the value network, it could be argued that the GaLUs and linear operations are entangled which in turn enable learning of sophisticated features in the layers. To demystify the layer-by-layer processing viewpoint, in the LGLN the value network is provided with a constant $\mathbf{1}$, i.e., if the input $x\in\R^{\din}$ is not given to the value network in the first place it is not possible to learn sophisticated structures in a layer-by-layer manner. Now, it could be argued that the gates are still arranged layer-by-layer, due to which, the network is able to still learn sophisticated structures in a layer-by-layer manner despite a constant $\mathbf{1}$ input. We show via experiments in \Cref{sec:exp} that destroying the layer-by-layer structure and the constant $\mathbf{1}$ does not degrade performance. These results are counter intuitive, surprising, and more importantly difficult to reconcile with using the `sophisticated features are learnt in a layer-by-layer' explanation. However, we argue that these surprising results can be readily explained using the NPK expression in \Cref{th:fcprev}.  Note that in a DGN, learning the weights with fixed gates amounts to learning a linear model in the dual variables, i.e., learning $\hat{y}_{\text{DGN}}(x)=\ip{\phi_{\Tf}(x), v_{\Tv}}$. We called this \textbf{dual linearity} and in \Cref{sec:prelim} showed that dual linearity has a kernel interpretation in terms of the NPK in \Cref{th:fcprev}. 
Thus, giving a constant $\mathbf{1}$ input would mean that $\ip{x,x'}=\din$, and the expression on the right reduces to $d \cdot \sigma^{2(d-1)}\cdot \din\cdot \textbf{overlap}(x,x')$. Now a previously unnoticed fact is that $\textbf{overlap}(x,x')$ is invariant to layer permutations, which implies destroying the layer-by-layer structure does not hurt. In short, the main role of the gating non-linearity is to lift the computations from the primal to the dual space. In the dual space, the right way to interpret the computations is path-by-path and the question of sophisticated structures being learnt in the layers is irrelevant.
%However, in terms of the dual variables, the constant $\mathbf{1}$ has the following interpretation: the neural path value is a vector specifying the contribution of each path to the final output and the neural path feature is a vector specifying the path activity (from \Cref{def:npf-npv} $\phi(x,p)=x\cdot A(x,p)$, and since input is $1$, we have $\phi(x,p)=A(x,p)$), i.e., which path to be selected and which one to be left out in the output. The destruction of layers is possible due to the permutation invariance property of the NPK itself which was unnoticed in prior work. 

In what follows, in \Cref{th:fc}, we first restate Theorem 5.1 in \cite{npk} (i.e., \Cref{th:fcprev}) so as to explicitly capture the permutation invariance. In order to build a  more complete picture, we extend the dual view to cover the cases of convolution with global pooling and skip connections. We then present the aforementioned experimental results in \Cref{sec:exp}.


%We now present additional theoretical and experimental insights on dual linearity that show that the once the gates are given the learning in the weights is best understood by a path-by-path view and the layer-by-layer view is not relevant.



%Given a DNN, there are two corresponding networks, the LGLN  in \Cref{sec:intro} and the DGN in \Cref{sec:prelim}. The LGLN can be obtained from the DGN by,

%(i) replacing all the ReLU activations in the feature network 

%The DGN performs only marginally poor compared to the DNN, and by successfully addressing the `black box'-ness issue in a DGN we will have both performance and interpretability. We now propose our novel \texttt{DGN-NO-ACT}  which disentangles the computations into (i) primal linear layer-by-layer computation, (ii) dual linear path-by-path computation and (iii) gating non-linearity which \emph{lifts} the computations from the primal to the dual. \texttt{DGN-NO-ACT} (right diagram in \Cref{fig:dgn}) is obtained by making a novel modification to the DGN (left diagram in \Cref{fig:dgn}) as follows:

%1. We replace the ReLUs in the feature network with \emph{identity} maps, i.e., $I(q) = q$. Thus, in the \texttt{DGN-NO-ACT}, the transformation from input to the pre-activations is entirely linear. We call this \textbf{primal linearity}. The disentanglement happens because the ReLU  is completely removed.  

%2. The value network of the DGN is linear in dual `path' variables. We call this \textbf{dual linearity} which stands for the fact that the value network computes path-by-path, and not layer-by-layer. As a result, presenting a $\mathbf{1}$ as input to the value network in \texttt{DGN-NO-ACT} does not degrade performance. The simplifications are : (i) $v_{\Tv}\in \R^{\text{total\,paths}}$ is a vector that does not depend on the input, and (ii) $\phi_{\Tf}(x)\in(0,1)^{\text{total\,paths}}$ is a  simple feature vector. In other words, the value network  learns the paths, and the feature network learns the activations of paths for each input.  While in each layer of the value network GaLUs and the linear operations are entangled, disentanglement happens in the path variables, and we do not have to worry about how learning happens layer-by-layer.

%3. The gates themselves serve the functionality of \emph{lifting} the computations from the primal to the dual, i.e., pre-activations trigger the gates which in turn activate the paths.

%\textbf{Significance.} The salient importance of the \texttt{DGN-NO-ACT} is that it clearly separates the layer-by-layer computations in the feature network and path-by-path computation in the value network. The importance of primal linearity is that we can now use standard linear algebra to `mathematically' interpret the linear transformations from the input to the pre-activations. Also, in the case of specific application domains such as `image classification', these transformations are `filter banks' which have been extensively studied with well known interpretations. The `mathematical' as well as domain specific interpretabilities obviate the need for `locally linear explanation using simpler models': the feature network is entirely linear and is itself simple. The importance of dual linearity is that it gives us a \emph{kernel} based `mathematical' interpretation in the \emph{infinite width regime}. 


%\textbf{DGN-NO-ACT(our model)}(see right diagram in \Cref{fig:dgn}). We replace the ReLUs in the feature network with \emph{identity} maps, i.e., $I(q) = q$. Thus, in the \texttt{DGN-NO-ACT}, the transformation from input to the pre-activations is entirely linear and is amenable to interpertation via standard spectral analysis using linear algebraic tools. The pre-activations trigger the gates, which then dictates the path activity. The value network is given only given $\mathbf{1}$ as input and hence $v_{\Tv}\in \R^{\text{total\,paths}}$ is a vector that does not depend on the input, and $\phi_{\Tf}(x)\in(0,1)^{\text{total\,paths}}$ is a very simple feature vector. % and the output $\hat{y}_{\text{DGN}}(x)=\ip{\phi_{\Tf}(x),v_{\Tv}}=\sum_{p}\phi_\Tf(x,p)v_\Tv(p)$ is equal to \emph{\textbf{the summation of `path value' weighted by the `path activations'}}. 
%Thus, the \texttt{DGN-NO-ACT} can be seen to disentangle the `primal linear' feature network which generates the gates, which in turn activate the paths in the value network which is the `dual linear'.

%\textbf{Remark.} Presenting the value network with $\mathbf{1}$ is very counter intutive, which will get justfied in theory as well as experiments in the next section. As a quick check, we the test accuracies of a DNN $4$ with convolutional layers and global-average-pooling (GAP), its corresponding DGN and \texttt{DGN-NO-ACT} on CIFAR-10 are {\bf{DNN: $80.4\%$,  DGN: $77.4\%$, \texttt{DGN-NO-ACT} : $74.5\%$}}.

%Since $\hat{y}_{\Theta}(x)=\ip{\phi_{x,\Theta},v_{\Theta}}$, during training, as $\Theta$ is learnt, both the NPFs and NPV are also learnt. To understand their roles better, $\phi_{x,\Theta}$ and $v_{\Theta}$ have to be separated.  This is achieved by the deep gated network (DGN) setup (see \Cref{fig:dgn}), which has two networks of \emph{identical architecture} namely the \emph{feature network} ($\Tf\in\R^{d^{\text{f}}_{\text{net}}}$) which holds the NPFs (i.e., the gating information) and the \emph{value network} ($\Tv\in\R^{d^{\text{v}}_{\text{net}}}$) which holds the NPV.  The combined parameterisation is denoted by $\Theta^{\text{DGN}}=(\Tf,\Tv)\in \R^{d^{\text{f}}_{\text{net}}+d^{\text{v}}_{\text{net}}}$.  The feature network is a DNN with ReLUs and the value network is a DNN with \emph{Gated Linear Units (GaLUs)} (terminology used in [\citenum{sss}]) whose output is the product of its pre-activation input $q^{\text{v}}(x)$and the external gating signal $G^{\text{f}}(x)$ (see \Cref{fig:dgn}). In \Cref{fig:dgn}, the main output of the DGN is $\hat{y}_{\text{DGN}}(x)$, while the other output $\hat{y}_{\text{f}}(x)$ is used to \emph{pre-train} the gates (see \Cref{sec:exp}).

